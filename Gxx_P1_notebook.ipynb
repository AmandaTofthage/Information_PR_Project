{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>PRI 2023/24: first project delivery</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GROUP 1**\n",
    "- Amanda Tofthagen, 113124\n",
    "- Tora Kristine Løtveit, 112927\n",
    "- Tuva Grønvold Natvig, 113107"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part I: demo of facilities</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/amandatofthagen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/amandatofthagen/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/amandatofthagen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/amandatofthagen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "import nltk  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing function and loading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lower case and tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in punctuation]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Load metadata as both dataframe and list\n",
    "def load_metadata(file_path):\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    df = df[['cord_uid', 'title', 'abstract']].dropna()  # Keep only required columns\n",
    "    df['title'] = df['title'].astype(str)\n",
    "    df['abstract'] = df['abstract'].astype(str)\n",
    "\n",
    "    # Store as a list of \"title + abstract\" for ranking models\n",
    "    doc_list = df.apply(lambda x: f\"{x['title']} {x['abstract']}\", axis=1).tolist()\n",
    "    \n",
    "    return df, doc_list  # Return both formats\n",
    "\n",
    "\n",
    "# Load qrels\n",
    "def load_qrels(file_path):\n",
    "    qrels = defaultdict(dict)\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            topic_id, _, doc_id, relevance = line.strip().split()\n",
    "            qrels[topic_id][doc_id] = int(relevance)\n",
    "    return qrels\n",
    "\n",
    "def load_queries(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    queries = {}\n",
    "    for topic in root.findall('topic'):\n",
    "        topic_number = topic.get('number')\n",
    "        query_text = preprocess_text(topic.find('query').text)\n",
    "        queries[topic_number] = \" \".join(query_text)  # Ensure consistency\n",
    "    return queries\n",
    "\n",
    "metadata_path = \"data/metadata.csv\"\n",
    "qrels_path = \"data/qrels.txt\"\n",
    "queries_path = \"data/topics.xml\"\n",
    "\n",
    "D, D_list = load_metadata(metadata_path)\n",
    "qrels = load_qrels(qrels_path)\n",
    "queries = load_queries(queries_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) **Indexing** (preprocessing and indexing options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing(D):\n",
    "    start_time = time.time()  # Start timing\n",
    "    \n",
    "    # Initialize the inverted index\n",
    "    inverted_index = defaultdict(dict)\n",
    "\n",
    "    # Process each document\n",
    "    for index, row in D.iterrows():\n",
    "        # Combine title and abstract for indexing\n",
    "        document_text = f\"{row['title']} {row['abstract']}\"\n",
    "        # Preprocess text\n",
    "        tokens = preprocess_text(document_text)\n",
    "\n",
    "        # Build the index\n",
    "        for term in tokens:\n",
    "            if index in inverted_index[term]:\n",
    "                inverted_index[term][index] += 1\n",
    "            else:\n",
    "                inverted_index[term][index] = 1\n",
    "\n",
    "    # Calculate time and space used\n",
    "    indexing_time = time.time() - start_time\n",
    "    index_size = sum(sum(freq.values()) for freq in inverted_index.values())  # Calculate the size of the index\n",
    "\n",
    "    # Return the inverted index, time taken, and estimated size of the index\n",
    "    return inverted_index, indexing_time, index_size\n",
    "\n",
    "def save_index_to_json(inverted_index, file_name='inverted_index.json'):\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(inverted_index, f, indent=4)\n",
    "\n",
    "def save_index_to_json(inverted_index, directory='output_data', file_name='inverted_index.json'):\n",
    "    # Check if the directory exists, if not create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Full path to save the file\n",
    "    path_to_file = os.path.join(directory, file_name)\n",
    "\n",
    "    # Save the JSON file\n",
    "    with open(path_to_file, 'w') as f:\n",
    "        json.dump(inverted_index, f, indent=4)\n",
    "\n",
    "# Run it:\n",
    "inverted_index, time_taken, size = indexing(D)\n",
    "save_index_to_json(inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_ir_model(q, k, I, queries, D):\n",
    "    \"\"\"\n",
    "    Performs Boolean Information Retrieval (IR) to rank documents.\n",
    "\n",
    "    Parameters:\n",
    "        q (str): The query/topic identifier.\n",
    "        k (int): The number of top documents to return.\n",
    "        I (dict): The inverted index.\n",
    "        queries (dict): Dictionary of preprocessed queries.\n",
    "        D (pd.DataFrame): Dataframe containing document information.\n",
    "\n",
    "    Returns:\n",
    "        list: Ordered list of document identifiers.\n",
    "    \"\"\"\n",
    "    query_terms = queries[q].split()\n",
    "\n",
    "    # Collect sets of document IDs for each term in the query\n",
    "    doc_sets = [set(I[term].keys()) for term in query_terms if term in I]\n",
    "\n",
    "    # If no terms from the query are in the index, return an empty list\n",
    "    if not doc_sets:\n",
    "        return []\n",
    "\n",
    "    # Boolean AND: Intersection of document sets containing all query terms\n",
    "    relevant_docs = set.intersection(*doc_sets)\n",
    "\n",
    "    # Rank documents based on term frequencies\n",
    "    doc_scores = {}\n",
    "    for doc_id in relevant_docs:\n",
    "        score = sum(I[term].get(doc_id, 0) for term in query_terms)\n",
    "        cord_uid = D.iloc[int(doc_id)]['cord_uid'] if int(doc_id) < len(D) else None\n",
    "        doc_scores[cord_uid] = score\n",
    "\n",
    "    # Sort documents by score in descending order\n",
    "    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return top k documents\n",
    "    top_docs = [doc_id for doc_id, _ in sorted_docs[:k]]\n",
    "    return top_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) **Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.1 Summarization solution: results for a given document*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_document(text, num_sentences=10):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"No valid text to summarize.\"\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return \"No content available for summarization.\"\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    except ValueError:\n",
    "        return \"Not enough content for summarization.\"\n",
    "    \n",
    "    scores = np.sum(tfidf_matrix.toarray(), axis=1)\n",
    "    ranked_sentences = [sentences[i] for i in np.argsort(scores)[-num_sentences:][::-1]]\n",
    "    \n",
    "    return ' '.join(ranked_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.2 IR models (TF-IDF, BM25 and EBRT)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(D):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(D)\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "def compute_bm25(D):\n",
    "    tokenized_corpus = [preprocess_text(doc) for doc in D]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    return bm25\n",
    "\n",
    "def ranking(q, p, I, method='tfidf', precomputed_bm25=None, precomputed_tfidf=None):\n",
    "    \"\"\"\n",
    "    Rank documents using TF-IDF, BM25, or BERT.\n",
    "    \n",
    "    :param q: Query string\n",
    "    :param p: Number of top documents to return\n",
    "    :param I: Inverted index (only used for TF-IDF)\n",
    "    :param method: Retrieval method ('tfidf', 'bm25', 'bert')\n",
    "    :param precomputed_bm25: Precomputed BM25 model (optional)\n",
    "    :param precomputed_tfidf: Tuple (TF-IDF matrix, vectorizer) if precomputed\n",
    "    :return: List of (document_id, score) pairs, sorted by relevance\n",
    "    \"\"\"\n",
    "    query_terms = preprocess_text(q)\n",
    "\n",
    "    if method == 'tfidf':\n",
    "        if precomputed_tfidf is None:\n",
    "            tfidf_matrix, vectorizer = compute_tfidf(D_list)\n",
    "        else:\n",
    "            tfidf_matrix, vectorizer = precomputed_tfidf\n",
    "        query_vector = vectorizer.transform([\" \".join(query_terms)])\n",
    "        scores = np.dot(tfidf_matrix, query_vector.T).toarray().flatten()\n",
    "        doc_scores = {doc_id: score for doc_id, score in zip(D['cord_uid'], scores)}\n",
    "\n",
    "    elif method == 'bm25':\n",
    "        if precomputed_bm25 is None:\n",
    "            bm25 = compute_bm25(D_list)\n",
    "        else:\n",
    "            bm25 = precomputed_bm25\n",
    "        scores = bm25.get_scores(query_terms)  \n",
    "        doc_scores = {doc_id: score for doc_id, score in zip(D['cord_uid'], scores)}\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid ranking method: Choose 'tfidf', 'bm25', or 'bert'\")\n",
    "\n",
    "    ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:p]\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.3 Reciprocal rank funsion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion Results: [('75773gwg', 0.0392156862745098), ('kn2z7lho', 0.038461538461538464), ('4fb291hq', 0.03773584905660377), ('hl967ekh', 0.037037037037037035), ('8ccl9aui', 0.03636363636363636)]\n"
     ]
    }
   ],
   "source": [
    "def reciprocal_rank_fusion(rankings, k=50):\n",
    "    fusion_scores = defaultdict(float)\n",
    "    for rank_list in rankings:\n",
    "        for rank, (doc, _) in enumerate(rank_list):\n",
    "            fusion_scores[doc] += 1 / (k + rank + 1)\n",
    "    return sorted(fusion_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Example usage:\n",
    "tfidf_results = ranking(queries[\"1\"], 5, inverted_index, 'bm25')\n",
    "bm25_results = ranking(queries[\"1\"], 5, inverted_index, 'bm25')\n",
    "\n",
    "fusion_results = reciprocal_rank_fusion([tfidf_results, bm25_results])\n",
    "print(\"Fusion Results:\", fusion_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.4 Maximal Marginal Relevance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximal_marginal_relevance(query, doc_vectors, vectorizer, lambda_param=0.5, top_n=5):\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    doc_similarities = cosine_similarity(doc_vectors, query_vector).flatten()\n",
    "    \n",
    "    selected = []\n",
    "    remaining = list(range(len(doc_similarities)))\n",
    "\n",
    "    selected_similarities = np.zeros(len(doc_similarities))\n",
    "\n",
    "    for _ in range(top_n):\n",
    "        if not remaining:\n",
    "            break\n",
    "\n",
    "        mmr_scores = lambda_param * doc_similarities[remaining] - (1 - lambda_param) * selected_similarities[remaining]\n",
    "\n",
    "        next_idx = remaining[np.argmax(mmr_scores)]\n",
    "        selected.append(next_idx)\n",
    "        remaining.remove(next_idx)\n",
    "\n",
    "        if remaining:\n",
    "            candidate_sims = cosine_similarity(doc_vectors[remaining], doc_vectors[next_idx]).flatten()\n",
    "            selected_similarities[remaining] = np.maximum(selected_similarities[remaining], candidate_sims)\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) **Keyword extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_tfidf(documents, top_n=10):\n",
    "    # Initialize TF-IDF Vectorizer \n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "    # Transform documents into TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    # Get list of all feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "   \n",
    "    top_keywords = {}\n",
    "    # Get the top N tf-idf scores for each document\n",
    "    for i, doc in enumerate(documents):\n",
    "        feature_index = tfidf_matrix[i,:].nonzero()[1]\n",
    "        tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
    "        sorted_items = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        top_keywords[i] = [(feature_names[idx], score) for idx, score in sorted_items]\n",
    "    \n",
    "    return top_keywords\n",
    "\n",
    "def extract_keywords_tfidf(documents, top_n=10):\n",
    "    # Initialize TF-IDF Vectorizer \n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "    # Transform documents into TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    # Get list of all feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "   \n",
    "    top_keywords = {}\n",
    "    # Get the top N tf-idf scores for each document\n",
    "    for i, doc in enumerate(documents):\n",
    "        feature_index = tfidf_matrix[i,:].nonzero()[1]\n",
    "        tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
    "        sorted_items = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        top_keywords[i] = [(feature_names[idx], score) for idx, score in sorted_items]\n",
    "    \n",
    "    return top_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D) **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precomputations for the evalution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompution for evaluation\n",
    "precomputed_bm25 = compute_bm25(D_list)  \n",
    "tfidf_matrix, tfidf_vectorizer = compute_tfidf(D_list)\n",
    "\n",
    "\n",
    "docs_text = (D['title'].fillna('') + \" \" + D['abstract'].fillna('')).tolist()\n",
    "mmr_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "doc_vectors = mmr_tfidf_matrix = mmr_vectorizer.fit_transform(docs_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠️ WARNING: 14 queries have no relevant documents in qrels.\n",
      "\n",
      "Evaluation Metrics:\n",
      "BM25: MAP=0.6022, nDCG@10=0.9379, P@10=0.9220, nDCG@1000=0.9058\n",
      "TFIDF: MAP=0.5395, nDCG@10=0.8086, P@10=0.7920, nDCG@1000=0.8777\n",
      "RRF: MAP=0.5534, nDCG@10=0.8908, P@10=0.8720, nDCG@1000=0.8002\n",
      "BOOLEAN: MAP=0.0256, nDCG@10=0.0705, P@10=0.0259, nDCG@1000=0.2674\n",
      "MMR: MAP=0.5446, nDCG@10=0.5446, P@10=0.7220, nDCG@1000=0.8138\n"
     ]
    }
   ],
   "source": [
    "def evaluation(Q, R, D, inverted_index, precomputed_bm25, precomputed_tfidf, precomputed_vectorizer, precomputed_doc_vectors):\n",
    "    metrics = {}\n",
    "    num_queries_missing_relevant_docs = 0\n",
    "\n",
    "    for method in ['bm25', 'tfidf', 'rrf', 'boolean', 'mmr']:\n",
    "        ndcg_10_vals, ndcg_1000_vals, p_10_vals, ap_vals = [], [], [], []\n",
    "\n",
    "        for qid, q_text in Q.items():\n",
    "            if method == 'bm25':\n",
    "                ranked_docs = ranking(q_text, 1000, inverted_index, method='bm25', precomputed_bm25=precomputed_bm25)\n",
    "            \n",
    "            elif method == 'tfidf':\n",
    "                ranked_docs = ranking(q_text, 1000, inverted_index, method='tfidf', precomputed_tfidf=precomputed_tfidf)\n",
    "            \n",
    "            elif method == 'rrf':\n",
    "                tfidf_results = ranking(q_text, 1000, inverted_index, method='tfidf', precomputed_tfidf=precomputed_tfidf)\n",
    "                bm25_results = ranking(q_text, 1000, inverted_index, method='bm25', precomputed_bm25=precomputed_bm25)\n",
    "                ranked_docs = reciprocal_rank_fusion([tfidf_results, bm25_results]) if tfidf_results and bm25_results else []\n",
    "            \n",
    "            elif method == 'boolean':\n",
    "                ranked_doc_ids = boolean_ir_model(qid, 1000, inverted_index, Q, D)\n",
    "                ranked_docs = [(doc_id, 1) for doc_id in ranked_doc_ids]\n",
    "            \n",
    "            elif method == 'mmr':\n",
    "                ranked_doc_indices = maximal_marginal_relevance(\n",
    "                    query=q_text,\n",
    "                    doc_vectors=mmr_tfidf_matrix,\n",
    "                    vectorizer=mmr_vectorizer,\n",
    "                    lambda_param=0.7,\n",
    "                    top_n=100\n",
    "                )\n",
    "                ranked_docs = [(D.iloc[idx]['cord_uid'], 1) for idx in ranked_doc_indices]\n",
    "\n",
    "            relevant_docs = [1 if doc[0] in R.get(qid, {}) else 0 for doc in ranked_docs]\n",
    "            scores = [doc[1] for doc in ranked_docs]\n",
    "\n",
    "            if sum(relevant_docs) == 0:\n",
    "                num_queries_missing_relevant_docs += 1\n",
    "                continue\n",
    "\n",
    "            ndcg_10_vals.append(ndcg_score([relevant_docs], [scores], k=10))\n",
    "            ndcg_1000_vals.append(ndcg_score([relevant_docs], [scores], k=1000))\n",
    "            ap_vals.append(average_precision_score(relevant_docs, scores))\n",
    "            p_10_vals.append(sum(relevant_docs[:10]) / min(10, len(relevant_docs)))\n",
    "\n",
    "        metrics[method] = {\n",
    "            \"MAP\": np.mean(ap_vals) if ap_vals else 0,\n",
    "            \"nDCG@10\": np.mean(ndcg_10_vals) if ndcg_10_vals else 0,\n",
    "            \"P@10\": np.mean(p_10_vals) if p_10_vals else 0,\n",
    "            \"nDCG@1000\": np.mean(ndcg_1000_vals) if ndcg_1000_vals else 0\n",
    "        }\n",
    "\n",
    "    if num_queries_missing_relevant_docs > 0:\n",
    "        print(f\"\\n⚠️ WARNING: {num_queries_missing_relevant_docs} queries have no relevant documents in qrels.\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Evaluate with precomputed vectors\n",
    "eval_metrics = evaluation(\n",
    "    queries, qrels, D, inverted_index, precomputed_bm25, \n",
    "    (tfidf_matrix, tfidf_vectorizer), mmr_vectorizer, mmr_tfidf_matrix\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "for method, scores in eval_metrics.items():\n",
    "    print(f\"{method.upper()}: MAP={scores['MAP']:.4f}, nDCG@10={scores['nDCG@10']:.4f}, P@10={scores['P@10']:.4f}, nDCG@1000={scores['nDCG@1000']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>END</H3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
