{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>PRI 2023/24: first project delivery</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GROUP 1**\n",
    "- Amanda Tofthagen, 113124\n",
    "- Tora Kristine Løtveit, 112927\n",
    "- Tuva Grønvold Natvig, 113107"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part I: demo of facilities</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/torakristinelotveit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/torakristinelotveit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/torakristinelotveit/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from tomlkit import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import xml.etree.ElementTree as ET\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "\n",
    "import string \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing function and loading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rz/79s4n049767_3h_h5sfqymym0000gn/T/ipykernel_27136/583744568.py:16: DtypeWarning: Columns (1,4,5,6,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lower case and tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Load Metadata as both dataframe and list\n",
    "def load_metadata(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[['cord_uid', 'title', 'abstract']].dropna()  # Keep only required columns\n",
    "    df['title'] = df['title'].astype(str)\n",
    "    df['abstract'] = df['abstract'].astype(str)\n",
    "\n",
    "    # Store as a list of \"title + abstract\" for ranking models\n",
    "    doc_list = df.apply(lambda x: f\"{x['title']} {x['abstract']}\", axis=1).tolist()\n",
    "    \n",
    "    return df, doc_list  # Return both formats\n",
    "\n",
    "\n",
    "# Load Qrels\n",
    "def load_qrels(file_path):\n",
    "    qrels = defaultdict(dict)\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            topic_id, _, doc_id, relevance = line.strip().split()\n",
    "            qrels[topic_id][doc_id] = int(relevance)\n",
    "    return qrels\n",
    "\n",
    "def load_queries(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    queries = {}\n",
    "    for topic in root.findall('topic'):\n",
    "        topic_number = topic.get('number')\n",
    "        query_text = preprocess_text(topic.find('query').text)\n",
    "        queries[topic_number] = \" \".join(query_text)  # Ensure consistency\n",
    "    return queries\n",
    "\n",
    "metadata_path = \"data/metadata.csv\"\n",
    "qrels_path = \"data/qrels.txt\"\n",
    "queries_path = \"data/topics.xml\"\n",
    "\n",
    "D, D_list = load_metadata(metadata_path)\n",
    "qrels = load_qrels(qrels_path)\n",
    "queries = load_queries(queries_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) **Indexing** (preprocessing and indexing options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def indexing(D):\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    # Initialize the inverted index\n",
    "    inverted_index = defaultdict(dict)\n",
    "\n",
    "    # Process each document\n",
    "    for index, row in D.iterrows():\n",
    "        # Combine title and abstract for indexing\n",
    "        document_text = f\"{row['title']} {row['abstract']}\"\n",
    "        # Preprocess text\n",
    "        tokens = preprocess_text(document_text)\n",
    "\n",
    "        # Build the index\n",
    "        for term in tokens:\n",
    "            if index in inverted_index[term]:\n",
    "                inverted_index[term][index] += 1\n",
    "            else:\n",
    "                inverted_index[term][index] = 1\n",
    "\n",
    "    # Calculate time and space used\n",
    "    indexing_time = time.time() - start_time\n",
    "    index_size = sum(sum(freq.values()) for freq in inverted_index.values())  # Calculate the size of the index\n",
    "\n",
    "    # Return the inverted index, time taken, and estimated size of the index\n",
    "    return inverted_index, indexing_time, index_size\n",
    "\n",
    "def save_index_to_json(inverted_index, file_name='inverted_index.json'):\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(inverted_index, f, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "def save_index_to_json(inverted_index, directory='output_data', file_name='inverted_index.json'):\n",
    "    # Check if the directory exists, if not create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Full path to save the file\n",
    "    path_to_file = os.path.join(directory, file_name)\n",
    "\n",
    "    # Save the JSON file\n",
    "    with open(path_to_file, 'w') as f:\n",
    "        json.dump(inverted_index, f, indent=4)\n",
    "\n",
    "# Example usage:\n",
    "inverted_index, time_taken, size = indexing(D)\n",
    "save_index_to_json(inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Query Results: [16, 22, 80, 93, 134]\n"
     ]
    }
   ],
   "source": [
    "def boolean_query(q, k, I, args=None):\n",
    "    \"\"\"\n",
    "    Perform a Boolean search on the inverted index.\n",
    "    q: Query string (text).\n",
    "    k: Number of top terms to consider.\n",
    "    I: Inverted index (dictionary).\n",
    "    args: Additional arguments (e.g., Boolean mode: 'AND' or 'OR').\n",
    "    \"\"\"\n",
    "    query_terms = preprocess_text(q)[:k]  # Use only the top-k terms\n",
    "    if not query_terms:\n",
    "        return []\n",
    "\n",
    "    boolean_mode = args.get('mode', 'AND') if args else 'AND'  # Default: AND search\n",
    "\n",
    "    # Retrieve document sets for each term in the query\n",
    "    doc_sets = [set(I[term].keys()) for term in query_terms if term in I]\n",
    "\n",
    "    if not doc_sets:\n",
    "        return []\n",
    "\n",
    "    # Apply Boolean logic\n",
    "    if boolean_mode == 'AND':\n",
    "        relevant_docs = set.intersection(*doc_sets) if doc_sets else set()\n",
    "    else:  # 'OR' mode (default if 'AND' fails)\n",
    "        relevant_docs = set.union(*doc_sets) if doc_sets else set()\n",
    "\n",
    "    return sorted(relevant_docs)[:k]  # Return up to `k` document IDs\n",
    "\n",
    "# Example usage:\n",
    "boolean_results = boolean_query(queries[\"1\"], 5, inverted_index, {'mode': 'OR'})\n",
    "print(\"Boolean Query Results:\", boolean_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) **Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.1 Summarization solution: results for a given document*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization:\n",
      "On the basis of biochemical evidence, it is often presumed that such NO• -dependent oxidations are due to the formation of the oxidant peroxynitrite, although alternative mechanisms involving the phagocyte-derived heme proteins myeloperoxidase and eosinophil peroxidase might be operative during conditions of inflammation. Inflammatory diseases of the respiratory tract are commonly associated with elevated production of nitric oxide (NO•) and increased indices of NO• -dependent oxidative stress. Although NO• is known to have anti-microbial, anti-inflammatory and anti-oxidant properties, various lines of evidence support the contribution of NO• to lung injury in several disease models.\n"
     ]
    }
   ],
   "source": [
    "def summarize_document(text, num_sentences=10):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"No valid text to summarize.\"\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return \"No content available for summarization.\"\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    except ValueError:\n",
    "        return \"Not enough content for summarization.\"\n",
    "    \n",
    "    scores = np.sum(tfidf_matrix.toarray(), axis=1)\n",
    "    ranked_sentences = [sentences[i] for i in np.argsort(scores)[-num_sentences:][::-1]]\n",
    "    \n",
    "    return ' '.join(ranked_sentences)\n",
    "\n",
    "#Example usage:\n",
    "\n",
    "print(\"Summarization:\")\n",
    "print(summarize_document(D_list[1], num_sentences=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.2 IR models (TF-IDF, BM25 and EBRT)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ranked_docs\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m bm25 \u001b[38;5;241m=\u001b[39m compute_bm25(D_list) \n\u001b[1;32m     60\u001b[0m ranked_docs \u001b[38;5;241m=\u001b[39m ranking(queries[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m5\u001b[39m, inverted_index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbm25\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBM25 Ranking:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ranked_docs)\n",
      "Cell \u001b[0;32mIn[52], line 7\u001b[0m, in \u001b[0;36mcompute_bm25\u001b[0;34m(D)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_bm25\u001b[39m(D):\n\u001b[0;32m----> 7\u001b[0m     tokenized_corpus \u001b[38;5;241m=\u001b[39m [preprocess_text(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m D]\n\u001b[1;32m      8\u001b[0m     bm25 \u001b[38;5;241m=\u001b[39m BM25Okapi(tokenized_corpus)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bm25\n",
      "Cell \u001b[0;32mIn[52], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_bm25\u001b[39m(D):\n\u001b[0;32m----> 7\u001b[0m     tokenized_corpus \u001b[38;5;241m=\u001b[39m [preprocess_text(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m D]\n\u001b[1;32m      8\u001b[0m     bm25 \u001b[38;5;241m=\u001b[39m BM25Okapi(tokenized_corpus)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bm25\n",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Convert text to lower case and tokenize\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Remove punctuation\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string\u001b[38;5;241m.\u001b[39mpunctuation]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/tokenize/destructive.py:181\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    178\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[0;32m--> 181\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS3:\n\u001b[1;32m    183\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def compute_tfidf(D):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(D)\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "def compute_bm25(D):\n",
    "    tokenized_corpus = [preprocess_text(doc) for doc in D]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    return bm25\n",
    "\n",
    "# Load Sentence-BERT model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def compute_bert(D):\n",
    "    embeddings = sbert_model.encode(D, convert_to_tensor=True)\n",
    "    return embeddings\n",
    "\n",
    "def ranking(q, p, I, method='tfidf', precomputed_bm25=None):\n",
    "    \"\"\"\n",
    "    Rank documents using TF-IDF, BM25, or BERT.\n",
    "    \n",
    "    :param q: Query string\n",
    "    :param p: Number of top documents to return\n",
    "    :param I: Inverted index (only used for TF-IDF)\n",
    "    :param method: Retrieval method ('tfidf', 'bm25', 'bert')\n",
    "    :param precomputed_bm25: Precomputed BM25 model (optional)\n",
    "    :return: List of (document_id, score) pairs, sorted by relevance\n",
    "    \"\"\"\n",
    "    query_terms = preprocess_text(q)\n",
    "\n",
    "    if method == 'tfidf':\n",
    "        tfidf_matrix, vectorizer = compute_tfidf(D_list)\n",
    "        query_vector = vectorizer.transform([\" \".join(query_terms)])\n",
    "        scores = np.dot(tfidf_matrix, query_vector.T).toarray().flatten()\n",
    "        doc_scores = {doc_id: score for doc_id, score in zip(D['cord_uid'], scores)}\n",
    "\n",
    "    elif method == 'bm25':\n",
    "        if precomputed_bm25 is None:\n",
    "            bm25 = compute_bm25(D_list)\n",
    "        else:\n",
    "            bm25 = precomputed_bm25\n",
    "        scores = bm25.get_scores(query_terms)  # Use precomputed model\n",
    "        doc_scores = {doc_id: score for doc_id, score in zip(D['cord_uid'], scores)}\n",
    "\n",
    "    elif method == 'bert':\n",
    "        doc_embeddings = compute_bert(D_list)\n",
    "        query_embedding = sbert_model.encode(q, convert_to_tensor=True)\n",
    "        scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)[0].cpu().numpy()\n",
    "        doc_scores = {doc_id: score for doc_id, score in zip(D['cord_uid'], scores)}\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid ranking method: Choose 'tfidf', 'bm25', or 'bert'\")\n",
    "\n",
    "    ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:p]\n",
    "    return ranked_docs\n",
    "\n",
    "# Example usage:\n",
    "bm25 = compute_bm25(D_list) \n",
    "\n",
    "ranked_docs = ranking(queries[\"1\"], 5, inverted_index, 'bm25')\n",
    "print(\"BM25 Ranking:\", ranked_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.3 Reciprocal rank funsion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion Results: [('75773gwg', 0.0392156862745098), ('kn2z7lho', 0.038461538461538464), ('4fb291hq', 0.03773584905660377), ('hl967ekh', 0.037037037037037035), ('8ccl9aui', 0.03636363636363636)]\n"
     ]
    }
   ],
   "source": [
    "def reciprocal_rank_fusion(rankings, k=50):\n",
    "    fusion_scores = defaultdict(float)\n",
    "    for rank_list in rankings:\n",
    "        for rank, (doc, _) in enumerate(rank_list):\n",
    "            fusion_scores[doc] += 1 / (k + rank + 1)\n",
    "    return sorted(fusion_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Example usage:\n",
    "tfidf_results = ranking(queries[\"1\"], 5, inverted_index, 'bm25')\n",
    "bm25_results = ranking(queries[\"1\"], 5, inverted_index, 'bm25')\n",
    "\n",
    "fusion_results = reciprocal_rank_fusion([tfidf_results, bm25_results])\n",
    "print(\"Fusion Results:\", fusion_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.4 Maximal Marginal Relevance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Marginal Relevance Results:\n",
      "1. Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia OBJECTIVE: This retrospective chart review describes the epidemiology a...\n",
      "2. Role of endothelin-1 in lung disease Endothelin-1 (ET-1) is a 21 amino acid peptide with diverse biological activity that has been implicated in numerous diseases. ET-1 is a potent mitogen regulator o...\n",
      "3. Sequence requirements for RNA strand transfer during nidovirus discontinuous subgenomic RNA synthesis Nidovirus subgenomic mRNAs contain a leader sequence derived from the 5′ end of the genome fused t...\n",
      "4. The 21st International Symposium on Intensive Care and Emergency Medicine, Brussels, Belgium, 20-23 March 2001 The 21st International Symposium on Intensive Care and Emergency Medicine was dominated b...\n",
      "5. Technical Description of RODS: A Real-time Public Health Surveillance System This report describes the design and implementation of the Real-time Outbreak and Disease Surveillance (RODS) system, a com...\n"
     ]
    }
   ],
   "source": [
    "def maximal_marginal_relevance(query, documents, lambda_param=0.5, top_n=5):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents + [query])\n",
    "    query_vector = tfidf_matrix[-1].toarray()[0]\n",
    "    doc_vectors = tfidf_matrix[:-1].toarray()\n",
    "    selected = []\n",
    "    remaining = list(range(len(documents)))\n",
    "    for _ in range(top_n):\n",
    "        if not remaining:\n",
    "            break\n",
    "        relevance_scores = [1 - cosine(query_vector, doc_vectors[i]) for i in remaining]\n",
    "        diversity_scores = [max([1 - cosine(doc_vectors[i], doc_vectors[j]) for j in selected] + [0]) for i in remaining]\n",
    "        mmr_scores = [lambda_param * rel - (1 - lambda_param) * div for rel, div in zip(relevance_scores, diversity_scores)]\n",
    "        best_doc = remaining[np.argmax(mmr_scores)]\n",
    "        selected.append(best_doc)\n",
    "        remaining.remove(best_doc)\n",
    "    return [documents[i] for i in selected]\n",
    "\n",
    "# Example usage:\n",
    "test_query = queries[\"1\"]  \n",
    "test_documents = D_list[:10]  \n",
    "mmr_results = maximal_marginal_relevance(test_query, test_documents, lambda_param=0.7, top_n=5)\n",
    "\n",
    "# Print Results\n",
    "print(\"Maximal Marginal Relevance Results:\")\n",
    "for i, doc in enumerate(mmr_results, 1):\n",
    "    print(f\"{i}. {doc[:200]}...\")  # Print only first 200 characters for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) **Keyword extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D) **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics: {'MAP': 0.967641975308642, 'nDCG@10': 0.9873204812903524, 'P@10': 0.9219999999999999}\n"
     ]
    }
   ],
   "source": [
    "def evaluation(Q, R, D, precomputed_bm25):\n",
    "    ndcg_values, map_values, precision_10 = [], [], []\n",
    "\n",
    "    for qid, q_text in Q.items():\n",
    "        ranked_docs = ranking(q_text, 10, inverted_index, method='bm25', precomputed_bm25=precomputed_bm25)\n",
    "\n",
    "\n",
    "        # Generate relevance labels (1 if doc is relevant, else 0)\n",
    "        relevant_docs = [1 if doc[0] in R.get(qid, {}) else 0 for doc in ranked_docs]\n",
    "\n",
    "        # Get document scores\n",
    "        scores = [doc[1] for doc in ranked_docs]\n",
    "\n",
    "        if len(relevant_docs) == 0 or len(scores) == 0:\n",
    "            continue  # Skip empty cases to avoid errors\n",
    "\n",
    "        # Ensure both lists have the same shape\n",
    "        ndcg_values.append(ndcg_score([relevant_docs], [scores]))  # No extra []\n",
    "        map_values.append(average_precision_score(relevant_docs, scores))  # 1D lists\n",
    "        precision_10.append(sum(relevant_docs[:10]) / min(10, len(relevant_docs)))  # Handle shorter lists\n",
    "\n",
    "    return {\n",
    "        \"MAP\": np.mean(map_values) if map_values else 0,\n",
    "        \"nDCG@10\": np.mean(ndcg_values) if ndcg_values else 0,\n",
    "        \"P@10\": np.mean(precision_10) if precision_10 else 0\n",
    "    }\n",
    "\n",
    "# Precompute BM25 Once\n",
    "precomputed_bm25 = compute_bm25(D_list)\n",
    "\n",
    "# Run Evaluation\n",
    "eval_metrics = evaluation(queries, qrels, D, precomputed_bm25)\n",
    "print(\"Evaluation Metrics:\", eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part II: questions materials (optional)</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Corpus *D* and summaries *S* description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Summarization performance for the overall and category-conditional corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**...** (additional questions with empirical results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>END</H3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
