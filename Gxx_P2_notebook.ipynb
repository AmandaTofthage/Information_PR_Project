{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>PRI 2023/24: second\n",
    "    project delivery</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GROUP 1**\n",
    "- Amanda Tofthagen, 113124\n",
    "- Tora Kristine LÃ¸tveit, 112927\n",
    "- Tuva GrÃ¸nvold Natvig, 113107"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Main facilities</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and preproccesing data (using with functions made in project 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/torakristinelotveit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/torakristinelotveit/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/torakristinelotveit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/torakristinelotveit/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "import nltk  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lower case and tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in punctuation]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Load metadata as both dataframe and list\n",
    "def load_metadata(file_path):\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    df = df[['cord_uid', 'title', 'abstract', 'authors', 'journal']].dropna()  # Keep only required columns\n",
    "    df['document'] = df.apply(lambda x: f\"{x['title']} {x['abstract']} {x['authors']} {x['journal']}\", axis=1)\n",
    "   \n",
    "    # Store as a list of \"title + abstract\" for ranking models\n",
    "    doc_list = df['document'].tolist()\n",
    "    \n",
    "    return df, doc_list  # Return both formats\n",
    "\n",
    "\n",
    "# Load qrels\n",
    "def load_qrels(file_path):\n",
    "    qrels = defaultdict(dict)\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            topic_id, _, doc_id, relevance = line.strip().split()\n",
    "            qrels[topic_id][doc_id] = int(relevance)\n",
    "    return qrels\n",
    "\n",
    "def load_queries(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    queries = {}\n",
    "    for topic in root.findall('topic'):\n",
    "        topic_number = topic.get('number')\n",
    "        query_text = preprocess_text(topic.find('query').text)\n",
    "        queries[topic_number] = \" \".join(query_text)  # Ensure consistency\n",
    "    return queries\n",
    "\n",
    "metadata_path = \"data2/metadata.csv\"\n",
    "qrels_path = \"data2/qrels.txt\"\n",
    "queries_path = \"data2/topics.xml\"\n",
    "\n",
    "D, D_list = load_metadata(metadata_path)\n",
    "qrels = load_qrels(qrels_path)\n",
    "queries = load_queries(queries_path)\n",
    "\n",
    "def indexing(D):\n",
    "    start_time = time.time()  # Start timing\n",
    "    \n",
    "    # Initialize the inverted index\n",
    "    inverted_index = defaultdict(dict)\n",
    "\n",
    "    # Process each document\n",
    "    for index, row in D.iterrows():\n",
    "        # Combine title and abstract for indexing\n",
    "        document_text = f\"{row['title']} {row['abstract']}\"\n",
    "        # Preprocess text\n",
    "        tokens = preprocess_text(document_text)\n",
    "\n",
    "        # Build the index\n",
    "        for term in tokens:\n",
    "            if index in inverted_index[term]:\n",
    "                inverted_index[term][index] += 1\n",
    "            else:\n",
    "                inverted_index[term][index] = 1\n",
    "\n",
    "    # Calculate time and space used\n",
    "    indexing_time = time.time() - start_time\n",
    "    index_size = sum(sum(freq.values()) for freq in inverted_index.values())  # Calculate the size of the index\n",
    "\n",
    "    # Return the inverted index, time taken, and estimated size of the index\n",
    "    return inverted_index, indexing_time, index_size\n",
    "\n",
    "def save_index_to_json(inverted_index, file_name='inverted_index.json'):\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(inverted_index, f, indent=4)\n",
    "\n",
    "def save_index_to_json(inverted_index, directory='output_data', file_name='inverted_index.json'):\n",
    "    # Check if the directory exists, if not create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Full path to save the file\n",
    "    path_to_file = os.path.join(directory, file_name)\n",
    "\n",
    "    # Save the JSON file\n",
    "    with open(path_to_file, 'w') as f:\n",
    "        json.dump(inverted_index, f, indent=4)\n",
    "\n",
    "# Run it:\n",
    "inverted_index, time_taken, size = indexing(D)\n",
    "save_index_to_json(inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part I: clustering</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A) Clustering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B) Summarization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*C) Keyword extraction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*D) Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part II: classification</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A) Feature extraction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract features from documents using TF-IDF\n",
    "def extract_features(doc_list):\n",
    "    processed_docs = []\n",
    "    for doc in doc_list:\n",
    "        tokens = preprocess_text(doc)  \n",
    "        processed_docs.append(\" \".join(tokens)) \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "# Usage\n",
    "tfidf_matrix, vectorizer = extract_features(D_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B) Classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Build and train\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m X, y \u001b[38;5;241m=\u001b[39m build_classification_dataset(qrels, queries, D, vectorizer)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of X:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositive samples:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(y))\n",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m, in \u001b[0;36mbuild_classification_dataset\u001b[0;34m(qrels, queries, D, vectorizer, max_neg_per_query)\u001b[0m\n\u001b[1;32m     11\u001b[0m cord_uid_to_index \u001b[38;5;241m=\u001b[39m {uid: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, uid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(D[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcord_uid\u001b[39m\u001b[38;5;124m'\u001b[39m])}\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Preprocess and vectorize all documents\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m processed_docs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(preprocess_text(text)) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m D[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     15\u001b[0m doc_vectors \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(processed_docs)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic_id, docs \u001b[38;5;129;01min\u001b[39;00m qrels\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m cord_uid_to_index \u001b[38;5;241m=\u001b[39m {uid: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, uid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(D[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcord_uid\u001b[39m\u001b[38;5;124m'\u001b[39m])}\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Preprocess and vectorize all documents\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m processed_docs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(preprocess_text(text)) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m D[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     15\u001b[0m doc_vectors \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(processed_docs)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic_id, docs \u001b[38;5;129;01min\u001b[39;00m qrels\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Convert text to lower case and tokenize\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Remove punctuation\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m punctuation]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/tokenize/destructive.py:181\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    178\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[0;32m--> 181\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS3:\n\u001b[1;32m    183\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Function to build a classification dataset (without BM25)\n",
    "def build_classification_dataset(qrels, queries, D, vectorizer, max_neg_per_query=20):\n",
    "    X, y = [], []\n",
    "\n",
    "    # Map cord_uid to row index\n",
    "    cord_uid_to_index = {uid: idx for idx, uid in enumerate(D['cord_uid'])}\n",
    "\n",
    "    # Preprocess and vectorize all documents\n",
    "    processed_docs = [\" \".join(preprocess_text(text)) for text in D['document']]\n",
    "    doc_vectors = vectorizer.transform(processed_docs)\n",
    "\n",
    "    for topic_id, docs in qrels.items():\n",
    "        if topic_id not in queries:\n",
    "            continue\n",
    "\n",
    "        query_text = queries[topic_id]\n",
    "        query_vec = vectorizer.transform([query_text])\n",
    "\n",
    "        # Positive samples\n",
    "        for doc_id, rel in docs.items():\n",
    "            doc_idx = cord_uid_to_index.get(doc_id)\n",
    "            if doc_idx is None:\n",
    "                continue\n",
    "            label = 1 if rel > 0 else 0\n",
    "            doc_vec = doc_vectors[doc_idx]\n",
    "            combined = 0.5 * query_vec + 0.5 * doc_vec\n",
    "            X.append(combined.toarray()[0])\n",
    "            y.append(label)\n",
    "\n",
    "        # Negative samples\n",
    "        neg_added = 0\n",
    "        for i, doc_id in enumerate(D['cord_uid']):\n",
    "            if doc_id in docs or neg_added >= max_neg_per_query:\n",
    "                continue\n",
    "            doc_vec = doc_vectors[i]\n",
    "            combined = 0.5 * query_vec + 0.5 * doc_vec\n",
    "            X.append(combined.toarray()[0])\n",
    "            y.append(0)\n",
    "            neg_added += 1\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Build and train\n",
    "X, y = build_classification_dataset(qrels, queries, D, vectorizer)\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Positive samples:\", sum(y))\n",
    "print(\"Negative samples:\", len(y) - sum(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n",
    "\n",
    "clf = LogisticRegression(max_iter=200, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*C) Ranking extension*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 reranked documents for query 1:\n",
      "Doc 29578 â†’ PR score: 0.005363\n",
      "cord_uid                                             wl121lg4\n",
      "title       Molecular immune pathogenesis and diagnosis of...\n",
      "Name: 41082, dtype: object\n",
      "Doc 28394 â†’ PR score: 0.004964\n",
      "cord_uid                                             wh9vvgv2\n",
      "title       T-cell immunity of SARS-CoV: Implications for ...\n",
      "Name: 39654, dtype: object\n",
      "Doc 35387 â†’ PR score: 0.004803\n",
      "cord_uid                                             msohf5oa\n",
      "title       The origin, transmission and clinical therapie...\n",
      "Name: 48161, dtype: object\n",
      "Doc 4163 â†’ PR score: 0.004793\n",
      "cord_uid                                             nj1p4ehx\n",
      "title       T-cell-mediated immune response to respiratory...\n",
      "Name: 4373, dtype: object\n",
      "Doc 26483 â†’ PR score: 0.004590\n",
      "cord_uid                                             4fs724wn\n",
      "title       Liver injury during highly pathogenic human co...\n",
      "Name: 34455, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "# Function to vectorize queries\n",
    "def vectorize_queries(queries_dict, vectorizer):\n",
    "    query_vectors = {}\n",
    "    for qid, query in queries_dict.items():\n",
    "        query_vec = vectorizer.transform([query]) \n",
    "        query_vectors[qid] = query_vec\n",
    "    return query_vectors\n",
    "\n",
    "# Function to get top-k documents for each query, based on cosine similarity\n",
    "def get_top_k_documents_per_query(query_vectors, doc_matrix, top_k=1000):\n",
    "    top_docs = {}\n",
    "    for qid, qvec in query_vectors.items():\n",
    "        sims = cosine_similarity(qvec, doc_matrix).flatten()\n",
    "        top_indices = np.argsort(sims)[::-1][:top_k]\n",
    "        top_scores = sims[top_indices]\n",
    "        top_docs[qid] = list(zip(top_indices, top_scores))  # (doc_id, similarity)\n",
    "    return top_docs\n",
    "\n",
    "\n",
    "# Function to build an undirected graph where edges are added between documents\n",
    "# based on cosine similarity above a certain threshold\n",
    "def build_graph(doc_indices, doc_matrix, theta=0.2):\n",
    "    G = nx.Graph()\n",
    "    docs_subset = doc_matrix[doc_indices]\n",
    "\n",
    "    sim_matrix = cosine_similarity(docs_subset)\n",
    "\n",
    "    for i, idx1 in enumerate(doc_indices):\n",
    "        for j, idx2 in enumerate(doc_indices):\n",
    "            if i < j and sim_matrix[i, j] >= theta:\n",
    "                G.add_edge(idx1, idx2, weight=sim_matrix[i, j])\n",
    "    return G\n",
    "\n",
    "# Function to run PageRank on the similarity graph\n",
    "def run_pagerank(graph, p=0.15, max_iter=100):\n",
    "    return nx.pagerank(graph, alpha=1 - p, max_iter=max_iter)\n",
    "\n",
    "# Function to rerank documents using PageRank based on importance\n",
    "# of documents in the similarity graph\n",
    "def undirected_page_rank(qid, tfidf_matrix, top_docs_per_query, theta=0.2):\n",
    "    if qid not in top_docs_per_query:\n",
    "        print(f\"Query {qid} not found.\")\n",
    "        return []\n",
    "\n",
    "    top_docs = [doc_id for doc_id, _ in top_docs_per_query[qid]]\n",
    "    G = build_graph(top_docs, tfidf_matrix, theta=theta)\n",
    "    scores = run_pagerank(G)\n",
    "\n",
    "    reranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return reranked\n",
    "\n",
    "\n",
    "# Function to rerank documents using PageRank with a combination of original score\n",
    "# and PageRank score\n",
    "def undirected_page_rank_fused(qid, tfidf_matrix, top_docs_per_query, theta=0.2, alpha=0.6):\n",
    "    if qid not in top_docs_per_query:\n",
    "        print(f\"Query {qid} not found.\")\n",
    "        return []\n",
    "\n",
    "    # Get top-1000 document indices\n",
    "    doc_indices = [doc_id for doc_id, _ in top_docs_per_query[qid]]\n",
    "    original_scores = dict(top_docs_per_query[qid])  # doc_id â†’ cosine_sim\n",
    "\n",
    "    # Build graph\n",
    "    G = build_graph(doc_indices, tfidf_matrix, theta=theta)\n",
    "    pr_scores = run_pagerank(G)\n",
    "\n",
    "    # Combine original score with PageRank\n",
    "    combined_scores = {\n",
    "        doc_id: alpha * pr_scores.get(doc_id, 0) + (1 - alpha) * original_scores.get(doc_id, 0)\n",
    "        for doc_id in doc_indices\n",
    "    }\n",
    "\n",
    "    reranked = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return reranked\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "# Step 1: Vectorize queries\n",
    "query_vectors = vectorize_queries(queries, vectorizer)\n",
    "\n",
    "# Step 2: Find top-1000 most similar docs per query\n",
    "top_docs_per_query = get_top_k_documents_per_query(query_vectors, tfidf_matrix)\n",
    "\n",
    "# Step 3: Rerank one query using PageRank\n",
    "qid = list(top_docs_per_query.keys())[0]\n",
    "reranked_docs = undirected_page_rank(qid, tfidf_matrix, top_docs_per_query, theta=0.2)\n",
    "\n",
    "# Show top 5 reranked docs\n",
    "print(f\"Top 5 reranked documents for query {qid}:\")\n",
    "for doc_id, score in reranked_docs[:5]:\n",
    "    print(f\"Doc {doc_id} â†’ PR score: {score:.6f}\")\n",
    "    print(D.iloc[doc_id][['cord_uid', 'title']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*D) Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Baseline Cosine Similarity Ranking (no graph):\n",
      "Evaluated 30 queries.\n",
      "Avg Precision@10: 0.3100\n",
      "Avg MAP:          0.2190\n",
      "Avg NDCG@10:      0.9926\n",
      "\n",
      "Evaluated 30 queries with Î¸ = 0.1\n",
      "Average Precision@10: 0.0767\n",
      "Average MAP: 0.0614\n",
      "Average NDCG@10: 0.9940\n",
      "\n",
      "ðŸ“Š Fused Ranking (alpha=0.6, Î¸=0.1):\n",
      "Avg Precision@10: 0.3100\n",
      "Avg MAP:          0.2199\n",
      "Avg NDCG@10:      0.9926\n",
      "\n",
      "Evaluated 30 queries with Î¸ = 0.2\n",
      "Average Precision@10: 0.0700\n",
      "Average MAP: 0.0571\n",
      "Average NDCG@10: 0.9940\n",
      "\n",
      "ðŸ“Š Fused Ranking (alpha=0.6, Î¸=0.2):\n",
      "Avg Precision@10: 0.3100\n",
      "Avg MAP:          0.2203\n",
      "Avg NDCG@10:      0.9926\n",
      "\n",
      "Evaluated 30 queries with Î¸ = 0.3\n",
      "Average Precision@10: 0.0667\n",
      "Average MAP: 0.0545\n",
      "Average NDCG@10: 0.9940\n",
      "\n",
      "ðŸ“Š Fused Ranking (alpha=0.6, Î¸=0.3):\n",
      "Avg Precision@10: 0.3100\n",
      "Avg MAP:          0.2205\n",
      "Avg NDCG@10:      0.9926\n",
      "\n",
      "Evaluated 30 queries with Î¸ = 0.4\n",
      "Average Precision@10: 0.0433\n",
      "Average MAP: 0.0575\n",
      "Average NDCG@10: 0.9940\n",
      "\n",
      "ðŸ“Š Fused Ranking (alpha=0.6, Î¸=0.4):\n",
      "Avg Precision@10: 0.3167\n",
      "Avg MAP:          0.2196\n",
      "Avg NDCG@10:      0.9926\n",
      "\n",
      "Evaluated 30 queries with Î¸ = 0.5\n",
      "Average Precision@10: 0.0467\n",
      "Average MAP: 0.0988\n",
      "Average NDCG@10: 0.9940\n",
      "\n",
      "ðŸ“Š Fused Ranking (alpha=0.6, Î¸=0.5):\n",
      "Avg Precision@10: 0.3133\n",
      "Avg MAP:          0.2201\n",
      "Avg NDCG@10:      0.9940\n",
      "\n",
      "Evaluated 30 queries with Î¸ = 0.6\n",
      "Average Precision@10: 0.0700\n",
      "Average MAP: 0.1563\n",
      "Average NDCG@10: 0.9940\n",
      "\n",
      "ðŸ“Š Fused Ranking (alpha=0.6, Î¸=0.6):\n",
      "Avg Precision@10: 0.3133\n",
      "Avg MAP:          0.2159\n",
      "Avg NDCG@10:      0.9926\n",
      "\n",
      "ðŸ“Š Fused Ranking (alpha=0.3, Î¸=0.2):\n",
      "Avg Precision@10: 0.3100\n",
      "Avg MAP:          0.2194\n",
      "Avg NDCG@10:      0.9926\n",
      "\n",
      "ðŸ“Š Fused Ranking (alpha=0.5, Î¸=0.2):\n",
      "Avg Precision@10: 0.3100\n",
      "Avg MAP:          0.2198\n",
      "Avg NDCG@10:      0.9926\n",
      "\n",
      "ðŸ“Š Fused Ranking (alpha=0.7, Î¸=0.2):\n",
      "Avg Precision@10: 0.3100\n",
      "Avg MAP:          0.2207\n",
      "Avg NDCG@10:      0.9926\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Function to get the true relevance scores for a given query from qrels\n",
    "def get_qrels_for_query(qid, qrels):\n",
    "    return qrels.get(qid, {})\n",
    "\n",
    "#Function to measure precision at k\n",
    "def precision_at_k(ranked_docs, relevant_docs, k=10):\n",
    "    top_k = ranked_docs[:k]\n",
    "    relevant = [1 if doc_id in relevant_docs and relevant_docs[doc_id] > 0 else 0 for doc_id in top_k]\n",
    "    return sum(relevant) / k\n",
    "\n",
    "# Function to calculate Mean Average Precision (MAP) for a ranked list of documents for a query\n",
    "def mean_average_precision(ranked_docs, relevant_docs):\n",
    "    num_relevant = 0\n",
    "    avg_precision = 0.0\n",
    "\n",
    "    for i, doc_id in enumerate(ranked_docs):\n",
    "        if doc_id in relevant_docs and relevant_docs[doc_id] > 0:\n",
    "            num_relevant += 1\n",
    "            avg_precision += num_relevant / (i + 1)\n",
    "\n",
    "    if num_relevant == 0:\n",
    "        return 0.0\n",
    "    return avg_precision / num_relevant\n",
    "\n",
    "# Function to calculate NDCG at k\n",
    "def ndcg_at_k(ranked_docs, relevant_docs, k=10):\n",
    "    true_rels = [relevant_docs.get(doc_id, 0) for doc_id in ranked_docs[:k]]\n",
    "    ideal_rels = sorted(relevant_docs.values(), reverse=True)[:k]\n",
    "\n",
    "    if not ideal_rels:\n",
    "        return 0.0\n",
    "\n",
    "    return ndcg_score([ideal_rels], [true_rels])\n",
    "\n",
    "# Function to evaluate the baseline cosine similarity ranking\n",
    "def evaluate_baseline_cosine(qrels, tfidf_matrix, query_vectors, D, top_k=1000):\n",
    "    D_reset = D.reset_index(drop=True)\n",
    "    index_to_uid = dict(enumerate(D_reset['cord_uid']))\n",
    "\n",
    "    all_precisions, all_maps, all_ndcgs = [], [], []\n",
    "    evaluated_queries = 0\n",
    "\n",
    "    for qid, qvec in query_vectors.items():\n",
    "        sims = cosine_similarity(qvec, tfidf_matrix).flatten()\n",
    "        top_indices = np.argsort(sims)[::-1][:top_k]\n",
    "        ranked_doc_ids = [index_to_uid[idx] for idx in top_indices]\n",
    "\n",
    "        relevant_docs = get_qrels_for_query(qid, qrels)\n",
    "        if not relevant_docs:\n",
    "            continue\n",
    "\n",
    "        p_at_10 = precision_at_k(ranked_doc_ids, relevant_docs, k=10)\n",
    "        avg_prec = mean_average_precision(ranked_doc_ids, relevant_docs)\n",
    "        ndcg = ndcg_at_k(ranked_doc_ids, relevant_docs, k=10)\n",
    "\n",
    "        all_precisions.append(p_at_10)\n",
    "        all_maps.append(avg_prec)\n",
    "        all_ndcgs.append(ndcg)\n",
    "        evaluated_queries += 1\n",
    "\n",
    "    print(f\"\\nðŸ“Š Baseline Cosine Similarity Ranking (no graph):\")\n",
    "    print(f\"Evaluated {evaluated_queries} queries.\")\n",
    "    print(f\"Avg Precision@10: {sum(all_precisions)/evaluated_queries:.4f}\")\n",
    "    print(f\"Avg MAP:          {sum(all_maps)/evaluated_queries:.4f}\")\n",
    "    print(f\"Avg NDCG@10:      {sum(all_ndcgs)/evaluated_queries:.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_all_queries_fused(qrels, tfidf_matrix, top_docs_per_query, D, alpha=0.6, theta=0.2):\n",
    "    D_reset = D.reset_index(drop=True)\n",
    "    index_to_uid = dict(enumerate(D_reset['cord_uid']))\n",
    "\n",
    "    all_precisions, all_maps, all_ndcgs = [], [], []\n",
    "    evaluated_queries = 0\n",
    "\n",
    "    for qid in top_docs_per_query:\n",
    "        # âš ï¸ Pass theta into your fused PageRank function\n",
    "        reranked = undirected_page_rank_fused(qid, tfidf_matrix, top_docs_per_query, alpha=alpha, theta=theta)\n",
    "        reranked_doc_ids = [index_to_uid[idx] for idx, _ in reranked]\n",
    "\n",
    "        relevant_docs = get_qrels_for_query(qid, qrels)\n",
    "        if not relevant_docs:\n",
    "            continue\n",
    "\n",
    "        p10 = precision_at_k(reranked_doc_ids, relevant_docs, k=10)\n",
    "        map_ = mean_average_precision(reranked_doc_ids, relevant_docs)\n",
    "        ndcg = ndcg_at_k(reranked_doc_ids, relevant_docs, k=10)\n",
    "\n",
    "        all_precisions.append(p10)\n",
    "        all_maps.append(map_)\n",
    "        all_ndcgs.append(ndcg)\n",
    "        evaluated_queries += 1\n",
    "\n",
    "    print(f\"\\nðŸ“Š Fused Ranking (alpha={alpha}, Î¸={theta}):\")\n",
    "    print(f\"Avg Precision@10: {sum(all_precisions)/evaluated_queries:.4f}\")\n",
    "    print(f\"Avg MAP:          {sum(all_maps)/evaluated_queries:.4f}\")\n",
    "    print(f\"Avg NDCG@10:      {sum(all_ndcgs)/evaluated_queries:.4f}\")\n",
    "\n",
    "\n",
    "# Ensure the DataFrame has a clean integer index\n",
    "D_reset = D.reset_index(drop=True)\n",
    "\n",
    "# Build index-to-uid map safely\n",
    "index_to_uid = dict(enumerate(D_reset['cord_uid']))\n",
    "reranked_doc_ids = [index_to_uid[idx] for idx, _ in reranked_docs]\n",
    "\n",
    "\n",
    "# Function to evaluate all queries\n",
    "def evaluate_all_queries(qrels, tfidf_matrix, top_docs_per_query, D, theta=0.2):\n",
    "    D_reset = D.reset_index(drop=True)\n",
    "    index_to_uid = dict(enumerate(D_reset['cord_uid']))\n",
    "\n",
    "    all_precisions = []\n",
    "    all_maps = []\n",
    "    all_ndcgs = []\n",
    "    evaluated_queries = 0\n",
    "\n",
    "    for qid in qrels.keys():\n",
    "        if qid not in top_docs_per_query:\n",
    "            continue\n",
    "\n",
    "        # Pass the theta value into the graph builder\n",
    "        reranked_docs = undirected_page_rank(qid, tfidf_matrix, top_docs_per_query, theta=theta)\n",
    "        reranked_doc_ids = [index_to_uid.get(idx) for idx, _ in reranked_docs if idx in index_to_uid]\n",
    "\n",
    "        relevant_docs = get_qrels_for_query(qid, qrels)\n",
    "        if not relevant_docs:\n",
    "            continue\n",
    "\n",
    "        p_at_10 = precision_at_k(reranked_doc_ids, relevant_docs, k=10)\n",
    "        avg_prec = mean_average_precision(reranked_doc_ids, relevant_docs)\n",
    "        ndcg = ndcg_at_k(reranked_doc_ids, relevant_docs, k=10)\n",
    "\n",
    "        all_precisions.append(p_at_10)\n",
    "        all_maps.append(avg_prec)\n",
    "        all_ndcgs.append(ndcg)\n",
    "        evaluated_queries += 1\n",
    "\n",
    "    avg_p10 = sum(all_precisions) / evaluated_queries if evaluated_queries > 0 else 0.0\n",
    "    avg_map = sum(all_maps) / evaluated_queries if evaluated_queries > 0 else 0.0\n",
    "    avg_ndcg = sum(all_ndcgs) / evaluated_queries if evaluated_queries > 0 else 0.0\n",
    "\n",
    "    print(f\"\\nEvaluated {evaluated_queries} queries with Î¸ = {theta}\")\n",
    "    print(f\"Average Precision@10: {avg_p10:.4f}\")\n",
    "    print(f\"Average MAP: {avg_map:.4f}\")\n",
    "    print(f\"Average NDCG@10: {avg_ndcg:.4f}\")\n",
    "\n",
    "\n",
    "#\n",
    "# Evaluate baseline cosine similarity ranking\n",
    "evaluate_baseline_cosine(qrels, tfidf_matrix, query_vectors, D)\n",
    "# Evaluate with different Î¸ values\n",
    "for theta in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]:\n",
    "    evaluate_all_queries(qrels, tfidf_matrix, top_docs_per_query, D, theta=theta)\n",
    "    evaluate_all_queries_fused(qrels, tfidf_matrix, top_docs_per_query, D, theta=theta)\n",
    "    # Evaluate the fused function with different Î± values\n",
    "best_theta = 0.2\n",
    "# Evaluate with different Î± values\n",
    "for alpha in [0.3, 0.5, 0.7]:\n",
    "    evaluate_all_queries_fused(qrels, tfidf_matrix, top_docs_per_query, D, alpha=alpha, theta=best_theta)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating Weighted PageRank with top_k_neighbors = 5\n",
      "\n",
      " Evaluated 30 queries in 29.4 seconds.\n",
      "Precision@10: 0.3133\n",
      "MAP:          0.2209\n",
      "NDCG@10:      0.9926\n",
      "\n",
      " Evaluating Weighted PageRank with top_k_neighbors = 10\n",
      "\n",
      " Evaluated 30 queries in 47.3 seconds.\n",
      "Precision@10: 0.3100\n",
      "MAP:          0.2206\n",
      "NDCG@10:      0.9926\n",
      "\n",
      " Evaluating Weighted PageRank with top_k_neighbors = 20\n",
      "\n",
      " Evaluated 30 queries in 95.0 seconds.\n",
      "Precision@10: 0.3100\n",
      "MAP:          0.2207\n",
      "NDCG@10:      0.9926\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "# Function to rerank documents using undirected PageRank with weighted edges and classifier-based priors\n",
    "def undirected_page_rank_weighted(\n",
    "    qid,\n",
    "    tfidf_matrix,\n",
    "    top_docs_per_query,\n",
    "    classifier=None,\n",
    "    doc_vectors=None,\n",
    "    query_vectors=None,\n",
    "    theta=0.2,\n",
    "    p=0.15,\n",
    "    max_iter=50,\n",
    "    top_k_neighbors=10,\n",
    "    use_cosine_fusion=False,\n",
    "    alpha=0.6\n",
    "):\n",
    "    if qid not in top_docs_per_query:\n",
    "        return []\n",
    "\n",
    "    doc_indices = [doc_id for doc_id, _ in top_docs_per_query[qid]]\n",
    "    docs_subset = tfidf_matrix[doc_indices]\n",
    "    sim_matrix = cosine_similarity(docs_subset)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for i, idx1 in enumerate(doc_indices):\n",
    "        sim_scores = list(enumerate(sim_matrix[i]))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        for j, score in sim_scores[:top_k_neighbors + 1]:\n",
    "            idx2 = doc_indices[j]\n",
    "            if i < j and score >= theta:\n",
    "                G.add_edge(idx1, idx2, weight=score)\n",
    "\n",
    "    if classifier and doc_vectors is not None and query_vectors is not None:\n",
    "        qvec = query_vectors[qid]\n",
    "        priors = {}\n",
    "        for i, idx in enumerate(doc_indices):\n",
    "            dvec = doc_vectors[idx]\n",
    "            combined = 0.5 * qvec + 0.5 * dvec\n",
    "            features = combined.toarray()\n",
    "            clf_score = classifier.predict_proba(features)[0][1]\n",
    "            priors[idx] = clf_score ** 2\n",
    "    else:\n",
    "        priors = {idx: 1.0 for idx in doc_indices}\n",
    "\n",
    "    total_prior = sum(priors.values())\n",
    "    priors = {k: v / total_prior for k, v in priors.items()}\n",
    "\n",
    "    PR = {node: 1 / len(G) for node in G.nodes()}\n",
    "    for _ in range(max_iter):\n",
    "        new_PR = {}\n",
    "        for node in G.nodes():\n",
    "            neighbors = list(G[node])\n",
    "            link_sum = 0.0\n",
    "            for neighbor in neighbors:\n",
    "                weight = G[neighbor][node]['weight']\n",
    "                total_weight = sum(G[neighbor][n]['weight'] for n in G[neighbor])\n",
    "                link_sum += PR[neighbor] * (weight / total_weight)\n",
    "            new_PR[node] = p * priors[node] + (1 - p) * link_sum\n",
    "        PR = new_PR\n",
    "\n",
    "    if use_cosine_fusion:\n",
    "        cosine_scores = dict(top_docs_per_query[qid])\n",
    "        reranked = [(doc_id, alpha * PR.get(doc_id, 0.0) + (1 - alpha) * cosine_scores.get(doc_id, 0.0))\n",
    "                    for doc_id in doc_indices]\n",
    "        reranked = sorted(reranked, key=lambda x: x[1], reverse=True)\n",
    "    else:\n",
    "        reranked = sorted(PR.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return reranked\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_all_queries_weighted(\n",
    "    qrels,\n",
    "    tfidf_matrix,\n",
    "    top_docs_per_query,\n",
    "    classifier,\n",
    "    doc_vectors,\n",
    "    query_vectors,\n",
    "    D,\n",
    "    theta=0.2,\n",
    "    p=0.15,\n",
    "    max_iter=50,\n",
    "    top_k_neighbors=10,\n",
    "    use_cosine_fusion=False,\n",
    "    alpha=0.6,\n",
    "    limit_queries=None\n",
    "):\n",
    "    from sklearn.metrics import ndcg_score\n",
    "\n",
    "    def get_qrels_for_query(qid, qrels):\n",
    "        return qrels.get(qid, {})\n",
    "\n",
    "    def precision_at_k(ranked_docs, relevant_docs, k=10):\n",
    "        top_k = ranked_docs[:k]\n",
    "        relevant = [1 if doc_id in relevant_docs and relevant_docs[doc_id] > 0 else 0 for doc_id in top_k]\n",
    "        return sum(relevant) / k\n",
    "\n",
    "    def mean_average_precision(ranked_docs, relevant_docs):\n",
    "        num_relevant = 0\n",
    "        avg_precision = 0.0\n",
    "        for i, doc_id in enumerate(ranked_docs):\n",
    "            if doc_id in relevant_docs and relevant_docs[doc_id] > 0:\n",
    "                num_relevant += 1\n",
    "                avg_precision += num_relevant / (i + 1)\n",
    "        return avg_precision / num_relevant if num_relevant > 0 else 0.0\n",
    "\n",
    "    def ndcg_at_k(ranked_docs, relevant_docs, k=10):\n",
    "        true_rels = [relevant_docs.get(doc_id, 0) for doc_id in ranked_docs[:k]]\n",
    "        ideal_rels = sorted(relevant_docs.values(), reverse=True)[:k]\n",
    "        return ndcg_score([ideal_rels], [true_rels]) if ideal_rels else 0.0\n",
    "\n",
    "    start = time.time()\n",
    "    D_reset = D.reset_index(drop=True)\n",
    "    index_to_uid = dict(enumerate(D_reset['cord_uid']))\n",
    "\n",
    "    all_precisions, all_maps, all_ndcgs = [], [], []\n",
    "    evaluated_queries = 0\n",
    "\n",
    "    for i, qid in enumerate(qrels.keys()):\n",
    "        if limit_queries and i >= limit_queries:\n",
    "            break\n",
    "        if qid not in top_docs_per_query:\n",
    "            continue\n",
    "\n",
    "        reranked_docs = undirected_page_rank_weighted(\n",
    "            qid=qid,\n",
    "            tfidf_matrix=tfidf_matrix,\n",
    "            top_docs_per_query=top_docs_per_query,\n",
    "            classifier=classifier,\n",
    "            doc_vectors=doc_vectors,\n",
    "            query_vectors=query_vectors,\n",
    "            theta=theta,\n",
    "            p=p,\n",
    "            max_iter=max_iter,\n",
    "            top_k_neighbors=top_k_neighbors,\n",
    "            use_cosine_fusion=use_cosine_fusion,\n",
    "            alpha=alpha\n",
    "        )\n",
    "\n",
    "        reranked_doc_ids = [index_to_uid.get(idx) for idx, _ in reranked_docs if idx in index_to_uid]\n",
    "        relevant_docs = get_qrels_for_query(qid, qrels)\n",
    "        if not relevant_docs:\n",
    "            continue\n",
    "\n",
    "        p10 = precision_at_k(reranked_doc_ids, relevant_docs)\n",
    "        map_ = mean_average_precision(reranked_doc_ids, relevant_docs)\n",
    "        ndcg = ndcg_at_k(reranked_doc_ids, relevant_docs)\n",
    "\n",
    "        all_precisions.append(p10)\n",
    "        all_maps.append(map_)\n",
    "        all_ndcgs.append(ndcg)\n",
    "        evaluated_queries += 1\n",
    "\n",
    "    print(f\"\\n Evaluated {evaluated_queries} queries in {time.time() - start:.1f} seconds.\")\n",
    "    print(f\"Precision@10: {np.mean(all_precisions):.4f}\")\n",
    "    print(f\"MAP:          {np.mean(all_maps):.4f}\")\n",
    "    print(f\"NDCG@10:      {np.mean(all_ndcgs):.4f}\")\n",
    "\n",
    "\n",
    "for k in [5, 10, 20]:\n",
    "    print(f\"\\n Evaluating Weighted PageRank with top_k_neighbors = {k}\")\n",
    "    evaluate_all_queries_weighted(\n",
    "        qrels=qrels,\n",
    "        tfidf_matrix=tfidf_matrix,\n",
    "        top_docs_per_query=top_docs_per_query,\n",
    "        classifier=clf,\n",
    "        doc_vectors=tfidf_matrix,\n",
    "        query_vectors=query_vectors,\n",
    "        D=D,\n",
    "        theta=0.2,\n",
    "        p=0.15,\n",
    "        max_iter=50,\n",
    "        top_k_neighbors=k,\n",
    "        use_cosine_fusion=True,\n",
    "        alpha=0.6\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Question materials (optional)</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part I: clustering</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Do clustering-guided summarization alters the behavior and efficacy of the IR system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** How sentence representations, clustering choices, and rank criteria impact summarization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**...** (additional questions with empirical results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>END</H3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
