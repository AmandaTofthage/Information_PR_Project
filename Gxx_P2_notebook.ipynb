{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>PRI 2023/24: second\n",
    "    project delivery</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GROUP 1**\n",
    "- Amanda Tofthagen, 113124\n",
    "- Tora Kristine Løtveit, 112927\n",
    "- Tuva Grønvold Natvig, 113107"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Main facilities</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and preproccesing data (using with functions made in project 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/torakristinelotveit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/torakristinelotveit/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/torakristinelotveit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/torakristinelotveit/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "import nltk  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lower case and tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in punctuation]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Load metadata as both dataframe and list\n",
    "def load_metadata(file_path):\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    df = df[['cord_uid', 'title', 'abstract']].dropna()  # Keep only required columns\n",
    "    df['title'] = df['title'].astype(str)\n",
    "    df['abstract'] = df['abstract'].astype(str)\n",
    "\n",
    "    # Store as a list of \"title + abstract\" for ranking models\n",
    "    doc_list = df.apply(lambda x: f\"{x['title']} {x['abstract']}\", axis=1).tolist()\n",
    "    \n",
    "    return df, doc_list  # Return both formats\n",
    "\n",
    "\n",
    "# Load qrels\n",
    "def load_qrels(file_path):\n",
    "    qrels = defaultdict(dict)\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            topic_id, _, doc_id, relevance = line.strip().split()\n",
    "            qrels[topic_id][doc_id] = int(relevance)\n",
    "    return qrels\n",
    "\n",
    "def load_queries(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    queries = {}\n",
    "    for topic in root.findall('topic'):\n",
    "        topic_number = topic.get('number')\n",
    "        query_text = preprocess_text(topic.find('query').text)\n",
    "        queries[topic_number] = \" \".join(query_text)  # Ensure consistency\n",
    "    return queries\n",
    "\n",
    "metadata_path = \"data2/metadata.csv\"\n",
    "qrels_path = \"data2/qrels.txt\"\n",
    "queries_path = \"data2/topics.xml\"\n",
    "\n",
    "D, D_list = load_metadata(metadata_path)\n",
    "qrels = load_qrels(qrels_path)\n",
    "queries = load_queries(queries_path)\n",
    "\n",
    "def indexing(D):\n",
    "    start_time = time.time()  # Start timing\n",
    "    \n",
    "    # Initialize the inverted index\n",
    "    inverted_index = defaultdict(dict)\n",
    "\n",
    "    # Process each document\n",
    "    for index, row in D.iterrows():\n",
    "        # Combine title and abstract for indexing\n",
    "        document_text = f\"{row['title']} {row['abstract']}\"\n",
    "        # Preprocess text\n",
    "        tokens = preprocess_text(document_text)\n",
    "\n",
    "        # Build the index\n",
    "        for term in tokens:\n",
    "            if index in inverted_index[term]:\n",
    "                inverted_index[term][index] += 1\n",
    "            else:\n",
    "                inverted_index[term][index] = 1\n",
    "\n",
    "    # Calculate time and space used\n",
    "    indexing_time = time.time() - start_time\n",
    "    index_size = sum(sum(freq.values()) for freq in inverted_index.values())  # Calculate the size of the index\n",
    "\n",
    "    # Return the inverted index, time taken, and estimated size of the index\n",
    "    return inverted_index, indexing_time, index_size\n",
    "\n",
    "def save_index_to_json(inverted_index, file_name='inverted_index.json'):\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(inverted_index, f, indent=4)\n",
    "\n",
    "def save_index_to_json(inverted_index, directory='output_data', file_name='inverted_index.json'):\n",
    "    # Check if the directory exists, if not create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Full path to save the file\n",
    "    path_to_file = os.path.join(directory, file_name)\n",
    "\n",
    "    # Save the JSON file\n",
    "    with open(path_to_file, 'w') as f:\n",
    "        json.dump(inverted_index, f, indent=4)\n",
    "\n",
    "# Run it:\n",
    "inverted_index, time_taken, size = indexing(D)\n",
    "save_index_to_json(inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part I: clustering</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A) Clustering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B) Summarization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*C) Keyword extraction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*D) Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part II: classification</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A) Feature extraction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract features from documents using TF-IDF\n",
    "def extract_features(doc_list):\n",
    "    processed_docs = []\n",
    "    for doc in doc_list:\n",
    "        tokens = preprocess_text(doc)  \n",
    "        processed_docs.append(\" \".join(tokens)) \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "# Usage\n",
    "tfidf_matrix, vectorizer = extract_features(D_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B) Classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (8965, 109895)\n",
      "Positive samples: 2096\n",
      "Negative samples: 6869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.73      0.80      1374\n",
      "           1       0.45      0.72      0.55       419\n",
      "\n",
      "    accuracy                           0.73      1793\n",
      "   macro avg       0.67      0.72      0.68      1793\n",
      "weighted avg       0.79      0.73      0.74      1793\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to build a classification dataset based on qrels and the document list\n",
    "def build_classification_dataset(qrels, queries, D, vectorizer, max_neg_per_query=50):\n",
    "    X, y = [], []\n",
    "\n",
    "    # Map cord_uid to row index\n",
    "    cord_uid_to_index = {uid: idx for idx, uid in enumerate(D['cord_uid'])}\n",
    "\n",
    "    # Precompute all document vectors\n",
    "    processed_docs = [\" \".join(preprocess_text(f\"{row['title']} {row['abstract']}\")) for _, row in D.iterrows()]\n",
    "    doc_vectors = vectorizer.transform(processed_docs)\n",
    "\n",
    "    for topic_id, docs in qrels.items():\n",
    "        if topic_id not in queries:\n",
    "            continue\n",
    "\n",
    "        query_text = queries[topic_id]\n",
    "        query_vec = vectorizer.transform([query_text])\n",
    "\n",
    "        # Positive samples\n",
    "        for doc_id, rel in docs.items():\n",
    "            doc_idx = cord_uid_to_index.get(doc_id)\n",
    "            if doc_idx is None:\n",
    "                continue\n",
    "            label = 1 if rel > 0 else 0\n",
    "            doc_vec = doc_vectors[doc_idx]\n",
    "            combined = 0.5 * query_vec + 0.5 * doc_vec\n",
    "            X.append(combined.toarray()[0])\n",
    "            y.append(label)\n",
    "\n",
    "        # Negative samples (not in qrels)\n",
    "        neg_added = 0\n",
    "        for i, doc_id in enumerate(D['cord_uid']):\n",
    "            if doc_id in docs or neg_added >= max_neg_per_query:\n",
    "                continue\n",
    "            doc_vec = doc_vectors[i]\n",
    "            combined = 0.5 * query_vec + 0.5 * doc_vec\n",
    "            X.append(combined.toarray()[0])\n",
    "            y.append(0)\n",
    "            neg_added += 1\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Build and train\n",
    "X, y = build_classification_dataset(qrels, queries, D, vectorizer)\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Positive samples:\", sum(y))\n",
    "print(\"Negative samples:\", len(y) - sum(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n",
    "\n",
    "clf = LogisticRegression(max_iter=200, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test) # Predicting on the test set\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*C) Ranking extension*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 reranked documents for query 1:\n",
      "Doc 33981 → PR score: 0.005286\n",
      "cord_uid                                             wl121lg4\n",
      "title       Molecular immune pathogenesis and diagnosis of...\n",
      "Name: 41082, dtype: object\n",
      "Doc 39810 → PR score: 0.004797\n",
      "cord_uid                                             msohf5oa\n",
      "title       The origin, transmission and clinical therapie...\n",
      "Name: 48161, dtype: object\n",
      "Doc 15165 → PR score: 0.004604\n",
      "cord_uid                                             52kqp9yw\n",
      "title       From SARS coronavirus to novel animal and huma...\n",
      "Name: 16201, dtype: object\n",
      "Doc 4168 → PR score: 0.004365\n",
      "cord_uid                                             nj1p4ehx\n",
      "title       T-cell-mediated immune response to respiratory...\n",
      "Name: 4373, dtype: object\n",
      "Doc 34462 → PR score: 0.004342\n",
      "cord_uid                                             kwq2y3il\n",
      "title       Coronavirus Disease 2019: Coronaviruses and Bl...\n",
      "Name: 41663, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "# Function to vectorize queries\n",
    "def vectorize_queries(queries_dict, vectorizer):\n",
    "    query_vectors = {}\n",
    "    for qid, query in queries_dict.items():\n",
    "        query_vec = vectorizer.transform([query]) \n",
    "        query_vectors[qid] = query_vec\n",
    "    return query_vectors\n",
    "\n",
    "# Function to get top-k documents for each query, based on cosine similarity\n",
    "def get_top_k_documents_per_query(query_vectors, doc_matrix, top_k=1000):\n",
    "    top_docs = {}\n",
    "    for qid, qvec in query_vectors.items():\n",
    "        sims = cosine_similarity(qvec, doc_matrix).flatten()\n",
    "        top_indices = np.argsort(sims)[::-1][:top_k]\n",
    "        top_scores = sims[top_indices]\n",
    "        top_docs[qid] = list(zip(top_indices, top_scores))  # (doc_id, similarity)\n",
    "    return top_docs\n",
    "\n",
    "\n",
    "# Function to build an undirected graph where edges are added between documents\n",
    "# based on cosine similarity above a certain threshold\n",
    "def build_graph(doc_indices, doc_matrix, theta=0.2):\n",
    "    G = nx.Graph()\n",
    "    docs_subset = doc_matrix[doc_indices]\n",
    "\n",
    "    sim_matrix = cosine_similarity(docs_subset)\n",
    "\n",
    "    for i, idx1 in enumerate(doc_indices):\n",
    "        for j, idx2 in enumerate(doc_indices):\n",
    "            if i < j and sim_matrix[i, j] >= theta:\n",
    "                G.add_edge(idx1, idx2, weight=sim_matrix[i, j])\n",
    "    return G\n",
    "\n",
    "# Function to run PageRank on the similarity graph\n",
    "def run_pagerank(graph, p=0.15, max_iter=50):\n",
    "    return nx.pagerank(graph, alpha=1 - p, max_iter=max_iter)\n",
    "\n",
    "# Function to rerank documents using PageRank based on importance\n",
    "# of documents in the similarity graph\n",
    "def undirected_page_rank(qid, tfidf_matrix, top_docs_per_query, theta=0.2):\n",
    "    if qid not in top_docs_per_query:\n",
    "        print(f\"Query {qid} not found.\")\n",
    "        return []\n",
    "\n",
    "    top_docs = [doc_id for doc_id, _ in top_docs_per_query[qid]]\n",
    "    G = build_graph(top_docs, tfidf_matrix, theta=theta)\n",
    "    scores = run_pagerank(G)\n",
    "\n",
    "    reranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return reranked\n",
    "\n",
    "\n",
    "# Function to rerank documents using PageRank with a combination of original score\n",
    "# and PageRank score\n",
    "def undirected_page_rank_fused(qid, tfidf_matrix, top_docs_per_query, theta=0.2, alpha=0.6):\n",
    "    if qid not in top_docs_per_query:\n",
    "        print(f\"Query {qid} not found.\")\n",
    "        return []\n",
    "\n",
    "    # Get top-1000 document indices\n",
    "    doc_indices = [doc_id for doc_id, _ in top_docs_per_query[qid]]\n",
    "    original_scores = dict(top_docs_per_query[qid])  # doc_id → cosine_sim\n",
    "\n",
    "    # Build graph\n",
    "    G = build_graph(doc_indices, tfidf_matrix, theta=theta)\n",
    "    pr_scores = run_pagerank(G)\n",
    "\n",
    "    # Combine original score with PageRank\n",
    "    combined_scores = {\n",
    "        doc_id: alpha * pr_scores.get(doc_id, 0) + (1 - alpha) * original_scores.get(doc_id, 0)\n",
    "        for doc_id in doc_indices\n",
    "    }\n",
    "\n",
    "    reranked = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return reranked\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "# Step 1: Vectorize queries\n",
    "query_vectors = vectorize_queries(queries, vectorizer)\n",
    "\n",
    "# Step 2: Find top-1000 most similar docs per query\n",
    "top_docs_per_query = get_top_k_documents_per_query(query_vectors, tfidf_matrix)\n",
    "\n",
    "# Step 3: Rerank one query using PageRank\n",
    "qid = list(top_docs_per_query.keys())[0]\n",
    "reranked_docs = undirected_page_rank(qid, tfidf_matrix, top_docs_per_query, theta=0.2)\n",
    "\n",
    "# Show top 5 reranked docs\n",
    "print(f\"Top 5 reranked documents for query {qid}:\")\n",
    "for doc_id, score in reranked_docs[:5]:\n",
    "    print(f\"Doc {doc_id} → PR score: {score:.6f}\")\n",
    "    print(D.iloc[doc_id][['cord_uid', 'title']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*D) Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Baseline Cosine Similarity Ranking (no graph):\n",
      "Evaluated 30 queries.\n",
      "Avg Precision@10: 0.3800\n",
      "Avg MAP:          0.2475\n",
      "Avg NDCG@10:      0.9943\n",
      "\n",
      "Evaluated 30 queries with θ = 0.1\n",
      "Average Precision@10: 0.1200\n",
      "Average MAP: 0.0876\n",
      "Average NDCG@10: 0.9940\n",
      "\n",
      "📊 Fused Ranking (alpha=0.6, θ=0.1):\n",
      "Avg Precision@10: 0.3833\n",
      "Avg MAP:          0.2482\n",
      "Avg NDCG@10:      0.9943\n",
      "\n",
      "Evaluated 30 queries with θ = 0.2\n",
      "Average Precision@10: 0.0900\n",
      "Average MAP: 0.0779\n",
      "Average NDCG@10: 0.9940\n",
      "\n",
      "📊 Fused Ranking (alpha=0.6, θ=0.2):\n",
      "Avg Precision@10: 0.3833\n",
      "Avg MAP:          0.2489\n",
      "Avg NDCG@10:      0.9943\n",
      "\n",
      "Evaluated 30 queries with θ = 0.3\n",
      "Average Precision@10: 0.0733\n",
      "Average MAP: 0.0682\n",
      "Average NDCG@10: 0.9940\n",
      "\n",
      "📊 Fused Ranking (alpha=0.6, θ=0.3):\n",
      "Avg Precision@10: 0.3833\n",
      "Avg MAP:          0.2483\n",
      "Avg NDCG@10:      0.9943\n",
      "\n",
      "Evaluated 30 queries with θ = 0.4\n",
      "Average Precision@10: 0.0633\n",
      "Average MAP: 0.0737\n",
      "Average NDCG@10: 0.9940\n",
      "\n",
      "📊 Fused Ranking (alpha=0.6, θ=0.4):\n",
      "Avg Precision@10: 0.3800\n",
      "Avg MAP:          0.2480\n",
      "Avg NDCG@10:      0.9943\n",
      "\n",
      "Evaluated 30 queries with θ = 0.5\n",
      "Average Precision@10: 0.0567\n",
      "Average MAP: 0.0955\n",
      "Average NDCG@10: 0.9926\n",
      "\n",
      "📊 Fused Ranking (alpha=0.6, θ=0.5):\n",
      "Avg Precision@10: 0.3800\n",
      "Avg MAP:          0.2485\n",
      "Avg NDCG@10:      0.9943\n",
      "\n",
      "Evaluated 30 queries with θ = 0.6\n",
      "Average Precision@10: 0.1033\n",
      "Average MAP: 0.1838\n",
      "Average NDCG@10: 0.9940\n",
      "\n",
      "📊 Fused Ranking (alpha=0.6, θ=0.6):\n",
      "Avg Precision@10: 0.3767\n",
      "Avg MAP:          0.2461\n",
      "Avg NDCG@10:      0.9943\n",
      "\n",
      "📊 Fused Ranking (alpha=0.3, θ=0.2):\n",
      "Avg Precision@10: 0.3800\n",
      "Avg MAP:          0.2479\n",
      "Avg NDCG@10:      0.9943\n",
      "\n",
      "📊 Fused Ranking (alpha=0.5, θ=0.2):\n",
      "Avg Precision@10: 0.3833\n",
      "Avg MAP:          0.2486\n",
      "Avg NDCG@10:      0.9943\n",
      "\n",
      "📊 Fused Ranking (alpha=0.7, θ=0.2):\n",
      "Avg Precision@10: 0.3833\n",
      "Avg MAP:          0.2492\n",
      "Avg NDCG@10:      0.9943\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Function to get the true relevance scores for a given query from qrels\n",
    "def get_qrels_for_query(qid, qrels):\n",
    "    return qrels.get(qid, {})\n",
    "\n",
    "#Function to measure precision at k\n",
    "def precision_at_k(ranked_docs, relevant_docs, k=10):\n",
    "    top_k = ranked_docs[:k]\n",
    "    relevant = [1 if doc_id in relevant_docs and relevant_docs[doc_id] > 0 else 0 for doc_id in top_k]\n",
    "    return sum(relevant) / k\n",
    "\n",
    "# Function to calculate Mean Average Precision (MAP) for a ranked list of documents for a query\n",
    "def mean_average_precision(ranked_docs, relevant_docs):\n",
    "    num_relevant = 0\n",
    "    avg_precision = 0.0\n",
    "\n",
    "    for i, doc_id in enumerate(ranked_docs):\n",
    "        if doc_id in relevant_docs and relevant_docs[doc_id] > 0:\n",
    "            num_relevant += 1\n",
    "            avg_precision += num_relevant / (i + 1)\n",
    "\n",
    "    if num_relevant == 0:\n",
    "        return 0.0\n",
    "    return avg_precision / num_relevant\n",
    "\n",
    "# Function to calculate NDCG at k\n",
    "def ndcg_at_k(ranked_docs, relevant_docs, k=10):\n",
    "    true_rels = [relevant_docs.get(doc_id, 0) for doc_id in ranked_docs[:k]]\n",
    "    ideal_rels = sorted(relevant_docs.values(), reverse=True)[:k]\n",
    "\n",
    "    if not ideal_rels:\n",
    "        return 0.0\n",
    "\n",
    "    return ndcg_score([ideal_rels], [true_rels])\n",
    "\n",
    "# Function to evaluate the baseline cosine similarity ranking\n",
    "def evaluate_baseline_cosine(qrels, tfidf_matrix, query_vectors, D, top_k=1000):\n",
    "    D_reset = D.reset_index(drop=True)\n",
    "    index_to_uid = dict(enumerate(D_reset['cord_uid']))\n",
    "\n",
    "    all_precisions, all_maps, all_ndcgs = [], [], []\n",
    "    evaluated_queries = 0\n",
    "\n",
    "    for qid, qvec in query_vectors.items():\n",
    "        sims = cosine_similarity(qvec, tfidf_matrix).flatten()\n",
    "        top_indices = np.argsort(sims)[::-1][:top_k]\n",
    "        ranked_doc_ids = [index_to_uid[idx] for idx in top_indices]\n",
    "\n",
    "        relevant_docs = get_qrels_for_query(qid, qrels)\n",
    "        if not relevant_docs:\n",
    "            continue\n",
    "\n",
    "        p_at_10 = precision_at_k(ranked_doc_ids, relevant_docs, k=10)\n",
    "        avg_prec = mean_average_precision(ranked_doc_ids, relevant_docs)\n",
    "        ndcg = ndcg_at_k(ranked_doc_ids, relevant_docs, k=10)\n",
    "\n",
    "        all_precisions.append(p_at_10)\n",
    "        all_maps.append(avg_prec)\n",
    "        all_ndcgs.append(ndcg)\n",
    "        evaluated_queries += 1\n",
    "\n",
    "    print(f\"\\n📊 Baseline Cosine Similarity Ranking (no graph):\")\n",
    "    print(f\"Evaluated {evaluated_queries} queries.\")\n",
    "    print(f\"Avg Precision@10: {sum(all_precisions)/evaluated_queries:.4f}\")\n",
    "    print(f\"Avg MAP:          {sum(all_maps)/evaluated_queries:.4f}\")\n",
    "    print(f\"Avg NDCG@10:      {sum(all_ndcgs)/evaluated_queries:.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_all_queries_fused(qrels, tfidf_matrix, top_docs_per_query, D, alpha=0.6, theta=0.2):\n",
    "    D_reset = D.reset_index(drop=True)\n",
    "    index_to_uid = dict(enumerate(D_reset['cord_uid']))\n",
    "\n",
    "    all_precisions, all_maps, all_ndcgs = [], [], []\n",
    "    evaluated_queries = 0\n",
    "\n",
    "    for qid in top_docs_per_query:\n",
    "        # ⚠️ Pass theta into your fused PageRank function\n",
    "        reranked = undirected_page_rank_fused(qid, tfidf_matrix, top_docs_per_query, alpha=alpha, theta=theta)\n",
    "        reranked_doc_ids = [index_to_uid[idx] for idx, _ in reranked]\n",
    "\n",
    "        relevant_docs = get_qrels_for_query(qid, qrels)\n",
    "        if not relevant_docs:\n",
    "            continue\n",
    "\n",
    "        p10 = precision_at_k(reranked_doc_ids, relevant_docs, k=10)\n",
    "        map_ = mean_average_precision(reranked_doc_ids, relevant_docs)\n",
    "        ndcg = ndcg_at_k(reranked_doc_ids, relevant_docs, k=10)\n",
    "\n",
    "        all_precisions.append(p10)\n",
    "        all_maps.append(map_)\n",
    "        all_ndcgs.append(ndcg)\n",
    "        evaluated_queries += 1\n",
    "\n",
    "    print(f\"\\n📊 Fused Ranking (alpha={alpha}, θ={theta}):\")\n",
    "    print(f\"Avg Precision@10: {sum(all_precisions)/evaluated_queries:.4f}\")\n",
    "    print(f\"Avg MAP:          {sum(all_maps)/evaluated_queries:.4f}\")\n",
    "    print(f\"Avg NDCG@10:      {sum(all_ndcgs)/evaluated_queries:.4f}\")\n",
    "\n",
    "\n",
    "# Ensure the DataFrame has a clean integer index\n",
    "D_reset = D.reset_index(drop=True)\n",
    "\n",
    "# Build index-to-uid map safely\n",
    "index_to_uid = dict(enumerate(D_reset['cord_uid']))\n",
    "reranked_doc_ids = [index_to_uid[idx] for idx, _ in reranked_docs]\n",
    "\n",
    "\n",
    "# Function to evaluate all queries\n",
    "def evaluate_all_queries(qrels, tfidf_matrix, top_docs_per_query, D, theta=0.2):\n",
    "    D_reset = D.reset_index(drop=True)\n",
    "    index_to_uid = dict(enumerate(D_reset['cord_uid']))\n",
    "\n",
    "    all_precisions = []\n",
    "    all_maps = []\n",
    "    all_ndcgs = []\n",
    "    evaluated_queries = 0\n",
    "\n",
    "    for qid in qrels.keys():\n",
    "        if qid not in top_docs_per_query:\n",
    "            continue\n",
    "\n",
    "        # Pass the theta value into the graph builder\n",
    "        reranked_docs = undirected_page_rank(qid, tfidf_matrix, top_docs_per_query, theta=theta)\n",
    "        reranked_doc_ids = [index_to_uid.get(idx) for idx, _ in reranked_docs if idx in index_to_uid]\n",
    "\n",
    "        relevant_docs = get_qrels_for_query(qid, qrels)\n",
    "        if not relevant_docs:\n",
    "            continue\n",
    "\n",
    "        p_at_10 = precision_at_k(reranked_doc_ids, relevant_docs, k=10)\n",
    "        avg_prec = mean_average_precision(reranked_doc_ids, relevant_docs)\n",
    "        ndcg = ndcg_at_k(reranked_doc_ids, relevant_docs, k=10)\n",
    "\n",
    "        all_precisions.append(p_at_10)\n",
    "        all_maps.append(avg_prec)\n",
    "        all_ndcgs.append(ndcg)\n",
    "        evaluated_queries += 1\n",
    "\n",
    "    avg_p10 = sum(all_precisions) / evaluated_queries if evaluated_queries > 0 else 0.0\n",
    "    avg_map = sum(all_maps) / evaluated_queries if evaluated_queries > 0 else 0.0\n",
    "    avg_ndcg = sum(all_ndcgs) / evaluated_queries if evaluated_queries > 0 else 0.0\n",
    "\n",
    "    print(f\"\\nEvaluated {evaluated_queries} queries with θ = {theta}\")\n",
    "    print(f\"Average Precision@10: {avg_p10:.4f}\")\n",
    "    print(f\"Average MAP: {avg_map:.4f}\")\n",
    "    print(f\"Average NDCG@10: {avg_ndcg:.4f}\")\n",
    "\n",
    "\n",
    "#\n",
    "# Evaluate baseline cosine similarity ranking\n",
    "evaluate_baseline_cosine(qrels, tfidf_matrix, query_vectors, D)\n",
    "# Evaluate with different θ values\n",
    "for theta in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]:\n",
    "    evaluate_all_queries(qrels, tfidf_matrix, top_docs_per_query, D, theta=theta)\n",
    "    evaluate_all_queries_fused(qrels, tfidf_matrix, top_docs_per_query, D, theta=theta)\n",
    "    # Evaluate the fused function with different α values\n",
    "best_theta = 0.2\n",
    "# Evaluate with different α values\n",
    "for alpha in [0.3, 0.5, 0.7]:\n",
    "    evaluate_all_queries_fused(qrels, tfidf_matrix, top_docs_per_query, D, alpha=alpha, theta=best_theta)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating weighted PageRank across queries...\n",
      "\n",
      "\n",
      "Evaluated 30 queries in 45.3 seconds.\n",
      "Average Precision@10: 0.3833\n",
      "Average MAP:          0.2506\n",
      "Average NDCG@10:      0.9943\n",
      "\n",
      "Testing: k=5, alpha=0.4\n",
      "Evaluating weighted PageRank across queries...\n",
      "\n",
      "\n",
      "Evaluated 30 queries in 24.3 seconds.\n",
      "Average Precision@10: 0.3833\n",
      "Average MAP:          0.2491\n",
      "Average NDCG@10:      0.9943\n",
      "\n",
      "Testing: k=5, alpha=0.6\n",
      "Evaluating weighted PageRank across queries...\n",
      "\n",
      "\n",
      "Evaluated 30 queries in 23.1 seconds.\n",
      "Average Precision@10: 0.3833\n",
      "Average MAP:          0.2515\n",
      "Average NDCG@10:      0.9943\n",
      "\n",
      "Testing: k=5, alpha=0.8\n",
      "Evaluating weighted PageRank across queries...\n",
      "\n",
      "\n",
      "Evaluated 30 queries in 23.6 seconds.\n",
      "Average Precision@10: 0.3933\n",
      "Average MAP:          0.2561\n",
      "Average NDCG@10:      0.9943\n",
      "\n",
      "Testing: k=10, alpha=0.4\n",
      "Evaluating weighted PageRank across queries...\n",
      "\n",
      "\n",
      "Evaluated 30 queries in 44.0 seconds.\n",
      "Average Precision@10: 0.3833\n",
      "Average MAP:          0.2487\n",
      "Average NDCG@10:      0.9943\n",
      "\n",
      "Testing: k=10, alpha=0.6\n",
      "Evaluating weighted PageRank across queries...\n",
      "\n",
      "\n",
      "Evaluated 30 queries in 45.3 seconds.\n",
      "Average Precision@10: 0.3833\n",
      "Average MAP:          0.2506\n",
      "Average NDCG@10:      0.9943\n",
      "\n",
      "Testing: k=10, alpha=0.8\n",
      "Evaluating weighted PageRank across queries...\n",
      "\n",
      "\n",
      "Evaluated 30 queries in 46.5 seconds.\n",
      "Average Precision@10: 0.3967\n",
      "Average MAP:          0.2548\n",
      "Average NDCG@10:      0.9943\n",
      "\n",
      "Testing: k=20, alpha=0.4\n",
      "Evaluating weighted PageRank across queries...\n",
      "\n",
      "\n",
      "Evaluated 30 queries in 104.0 seconds.\n",
      "Average Precision@10: 0.3833\n",
      "Average MAP:          0.2489\n",
      "Average NDCG@10:      0.9943\n",
      "\n",
      "Testing: k=20, alpha=0.6\n",
      "Evaluating weighted PageRank across queries...\n",
      "\n",
      "\n",
      "Evaluated 30 queries in 103.9 seconds.\n",
      "Average Precision@10: 0.3867\n",
      "Average MAP:          0.2503\n",
      "Average NDCG@10:      0.9943\n",
      "\n",
      "Testing: k=20, alpha=0.8\n",
      "Evaluating weighted PageRank across queries...\n",
      "\n",
      "\n",
      "Evaluated 30 queries in 104.5 seconds.\n",
      "Average Precision@10: 0.3967\n",
      "Average MAP:          0.2540\n",
      "Average NDCG@10:      0.9943\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "\n",
    "#Function to rerank documents using undirected PageRank with weighted edges\n",
    "# and classifier-based priors\n",
    "def undirected_page_rank_weighted(\n",
    "    qid,\n",
    "    tfidf_matrix,\n",
    "    top_docs_per_query,\n",
    "    classifier=None,\n",
    "    doc_vectors=None,\n",
    "    query_vectors=None,\n",
    "    D=None,\n",
    "    theta=0.2,\n",
    "    p=0.15,\n",
    "    max_iter=50,\n",
    "    top_k_neighbors=10,\n",
    "    use_cosine_fusion=False,\n",
    "    alpha=0.6\n",
    "):\n",
    "    if qid not in top_docs_per_query:\n",
    "        print(f\"[SKIP] Query {qid} not found in top_docs.\")\n",
    "        return []\n",
    "\n",
    "    doc_indices = [doc_id for doc_id, _ in top_docs_per_query[qid]]\n",
    "    docs_subset = tfidf_matrix[doc_indices]\n",
    "    sim_matrix = cosine_similarity(docs_subset)\n",
    "\n",
    "    # Build graph with top-k neighbors\n",
    "    G = nx.Graph()\n",
    "    for i, idx1 in enumerate(doc_indices):\n",
    "        sim_scores = list(enumerate(sim_matrix[i]))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        for j, score in sim_scores[:top_k_neighbors + 1]:\n",
    "            idx2 = doc_indices[j]\n",
    "            if i < j and score >= theta:\n",
    "                G.add_edge(idx1, idx2, weight=score)\n",
    "\n",
    "    if classifier and doc_vectors is not None and query_vectors is not None:\n",
    "        qvec = query_vectors[qid]\n",
    "        priors = {}\n",
    "        for i, idx in enumerate(doc_indices):\n",
    "            dvec = doc_vectors[idx]\n",
    "            combined = 0.5 * qvec + 0.5 * dvec\n",
    "            clf_score = classifier.predict_proba(combined)[0][1]\n",
    "            priors[idx] = clf_score ** 2  # emphasize strong confidence\n",
    "    else:\n",
    "        priors = {idx: 1.0 for idx in doc_indices}\n",
    "\n",
    "    # Normalize priors\n",
    "    total_prior = sum(priors.values())\n",
    "    priors = {k: v / total_prior for k, v in priors.items()}\n",
    "\n",
    "    # Weighted PageRank\n",
    "    PR = {node: 1 / len(G) for node in G.nodes()}\n",
    "    for _ in range(max_iter):\n",
    "        new_PR = {}\n",
    "        for node in G.nodes():\n",
    "            neighbors = list(G[node])\n",
    "            link_sum = 0.0\n",
    "            for neighbor in neighbors:\n",
    "                weight = G[neighbor][node]['weight']\n",
    "                total_neighbor_weight = sum(G[neighbor][n]['weight'] for n in G[neighbor])\n",
    "                link_sum += PR[neighbor] * (weight / total_neighbor_weight)\n",
    "            new_PR[node] = p * priors[node] + (1 - p) * link_sum\n",
    "        PR = new_PR\n",
    "\n",
    "    # Combine PR with cosine similarity if enabled\n",
    "    if use_cosine_fusion:\n",
    "        cosine_scores = dict(top_docs_per_query[qid])\n",
    "        reranked = [(doc_id, alpha * PR.get(doc_id, 0.0) + (1 - alpha) * cosine_scores.get(doc_id, 0.0))\n",
    "                    for doc_id in doc_indices]\n",
    "        reranked = sorted(reranked, key=lambda x: x[1], reverse=True)\n",
    "    else:\n",
    "        reranked = sorted(PR.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return reranked\n",
    "\n",
    "# Function to evaluate all queries using the weighted PageRank method\n",
    "def evaluate_all_queries_weighted(\n",
    "    qrels,\n",
    "    tfidf_matrix,\n",
    "    top_docs_per_query,\n",
    "    classifier,\n",
    "    doc_vectors,\n",
    "    query_vectors,\n",
    "    D,\n",
    "    theta=0.2,\n",
    "    p=0.15,\n",
    "    max_iter=50,\n",
    "    top_k_neighbors=10,\n",
    "    use_cosine_fusion=False,\n",
    "    alpha=0.6,\n",
    "    limit_queries=None\n",
    "):\n",
    "    start = time.time()\n",
    "    D_reset = D.reset_index(drop=True)\n",
    "    index_to_uid = dict(enumerate(D_reset['cord_uid']))\n",
    "\n",
    "    all_precisions, all_maps, all_ndcgs = [], [], []\n",
    "    evaluated_queries = 0\n",
    "\n",
    "    print(\"Evaluating weighted PageRank across queries...\\n\")\n",
    "    for i, qid in enumerate(qrels.keys()):\n",
    "        if limit_queries and i >= limit_queries:\n",
    "            break\n",
    "        if qid not in top_docs_per_query:\n",
    "            continue\n",
    "\n",
    "        reranked_docs = undirected_page_rank_weighted(\n",
    "            qid=qid,\n",
    "            tfidf_matrix=tfidf_matrix,\n",
    "            top_docs_per_query=top_docs_per_query,\n",
    "            classifier=classifier,\n",
    "            doc_vectors=doc_vectors,\n",
    "            query_vectors=query_vectors,\n",
    "            D=D,\n",
    "            theta=theta,\n",
    "            p=p,\n",
    "            max_iter=max_iter,\n",
    "            top_k_neighbors=top_k_neighbors,\n",
    "            use_cosine_fusion=use_cosine_fusion,\n",
    "            alpha=alpha\n",
    "        )\n",
    "\n",
    "        reranked_doc_ids = [index_to_uid.get(idx) for idx, _ in reranked_docs if idx in index_to_uid]\n",
    "        relevant_docs = get_qrels_for_query(qid, qrels)\n",
    "        if not relevant_docs:\n",
    "            continue\n",
    "\n",
    "        p_at_10 = precision_at_k(reranked_doc_ids, relevant_docs, k=10)\n",
    "        avg_prec = mean_average_precision(reranked_doc_ids, relevant_docs)\n",
    "        ndcg = ndcg_at_k(reranked_doc_ids, relevant_docs, k=10)\n",
    "\n",
    "        all_precisions.append(p_at_10)\n",
    "        all_maps.append(avg_prec)\n",
    "        all_ndcgs.append(ndcg)\n",
    "        evaluated_queries += 1\n",
    "\n",
    "    avg_p10 = sum(all_precisions) / evaluated_queries if evaluated_queries > 0 else 0.0\n",
    "    avg_map = sum(all_maps) / evaluated_queries if evaluated_queries > 0 else 0.0\n",
    "    avg_ndcg = sum(all_ndcgs) / evaluated_queries if evaluated_queries > 0 else 0.0\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"\\nEvaluated {evaluated_queries} queries in {end - start:.1f} seconds.\")\n",
    "    print(f\"Average Precision@10: {avg_p10:.4f}\")\n",
    "    print(f\"Average MAP:          {avg_map:.4f}\")\n",
    "    print(f\"Average NDCG@10:      {avg_ndcg:.4f}\")\n",
    "\n",
    "\n",
    "evaluate_all_queries_weighted(\n",
    "    qrels,\n",
    "    tfidf_matrix,\n",
    "    top_docs_per_query,\n",
    "    classifier=clf,\n",
    "    doc_vectors=tfidf_matrix,\n",
    "    query_vectors=query_vectors,\n",
    "    D=D,\n",
    "    theta=0.2,\n",
    "    p=0.15,\n",
    "    max_iter=50,\n",
    "    top_k_neighbors=10,\n",
    "    use_cosine_fusion=True,\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Explore the effect of top_k_neighbors and alpha\n",
    "for k in [5, 10, 20]:\n",
    "    for alpha in [0.4, 0.6, 0.8]:\n",
    "        print(f\"\\nTesting: k={k}, alpha={alpha}\")\n",
    "        evaluate_all_queries_weighted(\n",
    "            qrels=qrels,\n",
    "            tfidf_matrix=tfidf_matrix,\n",
    "            top_docs_per_query=top_docs_per_query,\n",
    "            classifier=clf,\n",
    "            doc_vectors=tfidf_matrix,\n",
    "            query_vectors=query_vectors,\n",
    "            D=D,\n",
    "            theta=0.2,\n",
    "            p=0.15,\n",
    "            max_iter=50,\n",
    "            top_k_neighbors=k,\n",
    "            use_cosine_fusion=True,\n",
    "            alpha=alpha\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Question materials (optional)</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part I: clustering</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Do clustering-guided summarization alters the behavior and efficacy of the IR system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** How sentence representations, clustering choices, and rank criteria impact summarization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**...** (additional questions with empirical results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>END</H3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
